---
title: "Influential Species — Pipeline transparency"
subtitle: "GBIF + NBN occurrences → merge → QC flags → policy filter → gridded presence/background"
author: "James Rimmer"
date: "2026-02-02"
format:
  html:
    css: pipeline_transparency.css
    toc: true
    toc-depth: 3
    number-sections: true
    theme: cosmo
    mermaid:
      theme: default
    code-fold: true
    code-tools: true
execute:
  echo: false
  warning: false
  message: false
---

## Purpose

This document explains, in plain English, what happens at each stage of the **InfluentialSpecies** workflow.

It is written to be readable for people who do not use R day-to-day, while still being specific enough that the main choices can be reviewed and revised.

## Pipeline overview

::: {.callout-note title="What the pipeline does"}
For each species, occurrence records are downloaded from **GBIF** (Europe-wide first pass) and the **UK NBN Atlas**. The workflow then:

1.  combines sources into a single per-species table,
2.  adds quality-control checks (these **flag** possible issues rather than deleting records),
3.  applies a modelling policy definition (an explicit “in/out” rule set), and
4.  converts the remaining point records into a gridded presence/background format.
:::

::: {.callout-tip title="Engine scripts and wrapper scripts"}
The repository uses two types of script:

-   **Engine scripts** in `R/` contain the core functions (the heavy lifting).
-   **Wrapper scripts** in `scripts/` are short and user-friendly. They set species lists and key settings, then call the engine functions.

This keeps the workflow reusable and makes it straightforward to run a small test set before scaling up.
:::

```{mermaid}
%%{init: {
  'theme': 'base',
  'themeVariables': { 'fontSize': '18px' },
  'flowchart': { 'nodeSpacing': 60, 'rankSpacing': 55 }
}}%%
flowchart TD
A["Stage 00: Download<br/>GBIF (Europe) + NBN (UK)"]
  --> B["Stage 01: Merge<br/>Combine sources; remove only safest 1-to-1 duplicates"]
B --> C["Stage 02: QC checks<br/>Add flags; do not delete records here"]
C --> D["Stage 03: Policy filter<br/>Apply defined 'in/out' rules for record removal"]
D --> E["Stage 04: Grid<br/>Convert points to grid (presence/background)"]
```

### Stages at a glance

| Stage | Plain-language description | Main output folder |
|----|----|----|
| **Stage 00 — Raw pull** | Download GBIF + NBN occurrence records for a species list and save them to disk | `data/raw/` |
| **Stage 01 — Merge** | Combine GBIF + NBN into a single per-species table (keeping provenance) | `data/processed/01_merged/` |
| **Stage 02 — QC flags** | Run QC checks and add flags (do not delete records here) | `data/processed/02_qc_flagged/` |
| **Stage 03 — Policy filter** | Apply a clear “in/out” policy for modelling | `data/processed/03_filtered/` |
| **Stage 04 — Grid** | Convert point records into a gridded presence/background representation | `data/processed/04_grid/` |

::: {.callout-note title="A note on species slugs"}
Outputs are stored in per-species folders using a **slug** version of the Latin name, for example:

-   `Myrmica sabuleti` → `myrmica_sabuleti`
:::

# Stage 00 — Raw occurrence pull (GBIF Europe + NBN UK)

::: {.callout-note title="What this stage does"}
Occurrence records are downloaded for a defined list of species from:

-   **GBIF** (Europe-wide first pass), and
-   **NBN Atlas** (UK).

Outputs are saved per species and per source as CSV files. These raw files are retained as the starting point for all later stages.
:::

::: {.callout-tip title="Where it is implemented"}
-   **Engine script:** `R/pull_raw_occurrences.R`
-   **Example wrapper:** `scripts/pull_raw_species_set_6sp_test_v_2.0.R` (test run)
:::

## Inputs

-   **Species list** (Latin names).
-   **NBN Atlas access** (via `galah`):
    -   the wrapper provides an **email address** used to identify requests,
    -   depending on the NBN endpoint used (particularly downloads), an NBN account login may also be required.
-   **GBIF access**:
    -   small/medium pulls can run without user credentials,
    -   larger pulls may use the GBIF download route (which can involve GBIF login details, depending on how the pull is configured).
-   **Cache setting**:
    -   whether to reuse existing raw outputs or force a fresh pull.

::: {.callout-important title="GBIF is handled in two ways (small vs large pulls)"}
GBIF can return very large numbers of records. This workflow supports two approaches:

-   **Smaller pulls (below \~100,000 rows):** a standard API query route.
-   **Large pulls (above \~100,000 rows):** an asynchronous **GBIF download** route.

The download route is designed for very large result sets and may involve authenticated downloads depending on configuration.
:::

::: {.callout-important title="Caching (what it does and why it exists)"}
Pulling data from GBIF and NBN can be slow, rate-limited, or occasionally fail due to temporary API issues.

-   When **caching is ON**, existing raw output files are reused if they already exist.
-   If the workflow has been updated to keep extra columns, older cached files that *lack required columns* are treated as out-of-date and are re-downloaded automatically.
-   When **caching is OFF**, the pull is forced each time.

This avoids repeated downloads and makes re-runs more reliable.
:::

## Outputs

-   `data/raw/gbif/<species_slug>/gbif_<species_slug>_clean.csv`
-   `data/raw/nbn/<species_slug>/nbn_<species_slug>_clean.csv`

## Key choices (Stage 00)

-   **GBIF spatial scope:** uses GBIF’s interpreted `continent = "EUROPE"` together with `hasCoordinate = TRUE` (first-pass, pragmatic extent).
-   **Column set retained:** the raw outputs retain not only coordinates and dates, but also record-type and provenance fields needed for later decisions (for example `basisOfRecord`, dataset information, and licensing fields).

::: {.callout-note title="Column lists written at Stage 00 (auto-detected from raw outputs on disk)" collapse="true"}
This block is included for transparency: it shows **exactly which columns are present** in the current raw outputs on the machine running the report.

It reads only the CSV headers (fast), and it will quietly do nothing if Stage 00 has not yet been run on this machine.

```{r}
suppressPackageStartupMessages({
  library(readr)
  library(tibble)
  library(dplyr)
})

find_first_file <- function(root, pattern_regex) {
  if (!dir.exists(root)) return(NA_character_)
  hits <- list.files(root, pattern = pattern_regex, recursive = TRUE, full.names = TRUE)
  if (length(hits) == 0) return(NA_character_)
  hits[1]
}

cols_from_csv_header <- function(path) {
  if (is.na(path) || !nzchar(path) || !file.exists(path)) return(character())
  df0 <- readr::read_csv(path, n_max = 0, show_col_types = FALSE, progress = FALSE)
  names(df0)
}

gbif_root <- file.path("data", "raw", "gbif")
nbn_root  <- file.path("data", "raw", "nbn")

# Use a regex that avoids awkward escaping:
gbif_example <- find_first_file(gbif_root, "_clean[.]csv$")
nbn_example  <- find_first_file(nbn_root,  "_clean[.]csv$")

gbif_cols <- cols_from_csv_header(gbif_example)
nbn_cols  <- cols_from_csv_header(nbn_example)

tibble(
  source = c("GBIF", "NBN"),
  example_file = c(ifelse(is.na(gbif_example), "(no example file found on disk)", gbif_example),
                   ifelse(is.na(nbn_example),  "(no example file found on disk)",  nbn_example)),
  n_columns = c(length(gbif_cols), length(nbn_cols))
) |> knitr::kable()

if (length(gbif_cols) > 0) {
  tibble(source = "GBIF", column = gbif_cols) |> knitr::kable()
} else {
  cat("\n**GBIF:** no raw file found yet on this machine.\n")
}

if (length(nbn_cols) > 0) {
  tibble(source = "NBN", column = nbn_cols) |> knitr::kable()
} else {
  cat("\n**NBN:** no raw file found yet on this machine.\n")
}
```
:::

## Notes on GBIF “Europe” filtering

GBIF’s `continent` field is interpreted (primarily from interpreted coordinates, otherwise interpreted country). Records with indeterminate continent (or records at sea) may not be assigned a continent value and can be excluded by this filter.

If this becomes an issue for a particular species (for example, coastal/island records), this can be replaced by an explicit coordinate-based extent.

# Stage 01 — Merge (GBIF + NBN combined)

::: {.callout-note title="What this stage does"}
Stage 01 combines GBIF and NBN into a single per-species table.

-   All original columns from both sources are retained (no early dropping).
-   A small set of derived fields are added to standardise identifiers, dates, and coordinate handling.
-   A very conservative cross-source duplicate rule is applied to remove only the safest 1-to-1 duplicates.
:::

::: {.callout-tip title="Where it is implemented"}
-   **Engine script:** `R/merge_dedup_occurrences.R` (function: `merge_occurrences()`)
-   **Example wrapper:** `scripts/merge_dedup_species_set_6sp_test.R`
:::

## Inputs

-   Stage 00 raw CSVs (GBIF and/or NBN) for each species.
-   Species list.
-   Duplicate-rule settings and refresh behaviour.

## Outputs

Per species:

-   `data/processed/01_merged/<species_slug>/occ_<species_slug>__merged.parquet` *(or `.rds` if Parquet support is not available)*

Run log:

-   `data/processed/01_merged/_runlog_01_merged.csv`

## Key choices (Stage 01)

### 01A — Conservative cross-source duplicate removal

Only the safest duplicates are removed, and only when *all* of the following are true:

-   there is **exactly 1 GBIF record and 1 NBN record** involved,
-   the pair share the **same rounded coordinates** (precision controlled by `coord_round_dp`), and
-   both records contain a **true day-level date** (`YYYY-MM-DD`), not a date range/interval.

When such a 1-to-1 pair is detected, the record from the **non-preferred source** is removed. The preferred source is controlled by `prefer_source` (commonly `"GBIF"`).

This is deliberately not “full de-duplication”. It removes only the most obvious cross-source duplicates and leaves all other cases untouched for later decisions.

### 01B — Reading raw CSVs safely (type handling)

Raw CSV inputs are read with **all columns forced to character** during ingestion. This avoids type-guess issues (especially around mixed date formats) and keeps the raw values intact.

### 01C — Running when one source is missing

Stage 01 can run even if only one source is available for a species (for example, when a large GBIF download is still pending). The run log notes this.

### 01D — Derived fields added at this stage

Stage 01 adds a small set of extra columns used downstream. Examples include:

-   identifiers: `species_slug`, `species_input`, `source`, `source_record_id`, `occ_uid`
-   date handling: `date_raw`, `date_is_range`, `event_day`, `year`, `date_precision`
-   coordinate handling: `lon`, `lat`, `lon_r`, `lat_r`
-   duplicate key: `key_day`

# Stage 02 — QC flagging (add flags; do not drop)

::: {.callout-note title="What this stage does"}
Stage 02 runs a set of quality checks and adds **QC flag columns** to the merged table.

No records are removed at this stage. The intent is to label potential problems so they can be filtered later (and so summaries can be produced without losing information early).
:::

::: {.callout-tip title="Where it is implemented"}
-   **Engine script:** `R/qc_flag_occurrences.R` (function: `qc_flag_occurrences()`)
-   **Example wrapper:** `scripts/qc_flag_species_set_6sp_test.R`
:::

## Inputs

-   Stage 01 merged per-species outputs (`01_merged`).
-   Species list.
-   QC thresholds and switches.

## Outputs

Per species:

-   `data/processed/02_qc_flagged/<species_slug>/occ_<species_slug>__qc_flagged.parquet` *(or `.rds` if Parquet support is not available)*

Run log:

-   `data/processed/02_qc_flagged/_runlog_02_qc_flagged.csv`

## Key choices (Stage 02)

### 02A — What is flagged

The current QC flags include:

| Flag | Meaning (plain English) |
|----|----|
| `qc_flag_missing_coords` | longitude or latitude is missing |
| `qc_flag_coords_out_of_range` | coordinates exist but are outside valid ranges |
| `qc_flag_missing_date` | no day-level date and no year available |
| `qc_flag_future_date` | day-level date is in the future, or year is in the future |
| `qc_flag_uncertainty_high` | coordinate uncertainty is greater than a chosen threshold |
| `qc_flag_unexpected_licence` | licensing field indicates the record is not in an expected licensing state |
| `qc_flag_has_issues` | an `issues` field is present and non-empty (primarily GBIF-style) |
| `qc_flag_any` | TRUE if any of the above flags are TRUE |
| `qc_flag_count` | number of TRUE flags per record (optional summary column) |

### 02B — Thresholds and switches

The main threshold is:

-   `max_coord_uncertainty_m` (default **10,000 metres**) used by `qc_flag_uncertainty_high`.

Two checks are controlled by switches, because they depend on whether those fields exist for a given source/pull configuration:

-   `flag_if_unexpected_licence`
-   `flag_if_has_issues`

# Stage 03 — Policy filter (to fill)

::: {.callout-note title="What this stage will describe"}
Stage 03 applies an explicit modelling policy (an “in/out” definition) to decide which records are retained for modelling.

This is where decisions such as “in situ observations only” (especially for GBIF) are implemented.
:::

-   **Engine script:** (to fill)
-   **Wrapper script:** (to fill)
-   **Outputs:** `data/processed/03_filtered/<species_slug>/...`

# Stage 04 — Grid (to fill)

::: {.callout-note title="What this stage will describe"}
Stage 04 converts point records into a gridded presence/background representation.

This includes choices such as the grid resolution (e.g. 25 km), coordinate system, modelling extent, and any land masking.
:::

-   **Engine script:** (to fill)
-   **Wrapper script:** (to fill)
-   **Outputs:** `data/processed/04_grid/<policy_tag>/...`
