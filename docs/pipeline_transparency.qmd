---
title: "Influential Species — Pipeline transparency"
subtitle: "GBIF + NBN occurrences → merge → QC flags → policy filter → gridded presence/background"
author: "James Rimmer"
date: "2026-02-02"
format:
  html:
    theme: cosmo
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
    code-tools: true
    mermaid:
      theme: default
    css: pipeline_transparency.css
execute:
  echo: false
  message: false
  warning: false
---

## Purpose

This document is a plain-language record of the key choices made in the **InfluentialSpecies** repository pipeline.

It is written to be understandable to someone who has never used R. Script names are provided for traceability, but the focus is on *what decisions are being made and why*.

## Pipeline overview

::: {.callout-note title="What the pipeline does"}
For each species, occurrence records are downloaded from **GBIF** (Europe-wide first pass) and the **UK NBN Atlas**. The workflow then:

1.  combines sources into a single per-species table,
2.  adds quality-control checks (these flag possible issues rather than deleting records),
3.  applies a modelling policy definition (an explicit “in/out” rule set), and
4.  converts the remaining point records into a gridded presence/background format.
:::

::: {.callout-tip title="Engine scripts and wrapper scripts"}
Two script types are used:

-   **Engine scripts** (in `R/`) contain core functions (the heavy lifting).
-   **Wrapper scripts** (in `scripts/`) are short and user-friendly. They set species lists and key settings, then call the engine functions.

This keeps the workflow reusable and makes it straightforward to run a small test set before scaling up.
:::

### Workflow diagram

```{mermaid}
%%{init: {"theme":"base","themeVariables":{"fontSize":"18px"},"flowchart":{"curve":"linear","nodeSpacing":60,"rankSpacing":90}}}%%
flowchart TB
  A["Stage 00 — Raw pull<br/>Download GBIF (Europe) + NBN (UK)"] --> B["Stage 01 — Merge<br/>Combine sources; remove only safest 1-to-1 duplicates"]
  B --> C["Stage 02 — QC flags<br/>Add QC flags; do not delete records here"]
  C --> D["Stage 03 — Policy filter<br/>Apply modelling policy (explicit 'in/out' rules)"]
  D --> E["Stage 04 — Grid<br/>Convert points to grid (presence/background)"]
```

::: {.callout-tip title="Making the diagram larger (CSS)"}
This document uses `pipeline_transparency.css` to control diagram size and spacing.

Create this file in the same folder as this `.qmd` (typically `docs/pipeline_transparency.css`) and include:

``` css
/* pipeline_transparency.css */

/* Make mermaid diagrams larger and easier to read */
.mermaid {
  font-size: 18px;
}

/* Ensure the SVG uses the available width */
.mermaid svg {
  width: 100% !important;
  height: auto !important;
}

/* Slightly increase line thickness for readability */
.mermaid .edgePath path {
  stroke-width: 2px !important;
}
```
:::

## Stages at a glance

| Stage | Plain-language description | Main output folder |
|------------------------|------------------------|------------------------|
| **Stage 00 — Raw pull** | Download GBIF + NBN occurrence records for a species | `data/raw/` |
| **Stage 01 — Merge** | Combine GBIF + NBN into one table per species; drop only the safest 1-to-1 duplicates | `data/processed/01_merged/` |
| **Stage 02 — QC flags** | Add QC flag columns (no deletion here) | `data/processed/02_qc_flagged/` |
| **Stage 03 — Policy filter** | Apply an explicit modelling policy (the “in/out” definition) | `data/processed/03_filtered/` |
| **Stage 04 — Grid** | Convert points into gridded presence/background | `data/processed/04_grid/` |

------------------------------------------------------------------------

# Stage 00 — Raw pull

::: {.callout-note title="What this stage does"}
Stage 00 downloads raw occurrence data from two sources:

-   **GBIF**: Europe-wide first pass (via the GBIF API)
-   **UK NBN Atlas**: UK-only (via `galah`)

It writes a “raw-clean” file per species per source, with basic coordinate usability checks and only exact within-source duplicate removal.
:::

-   **Engine script:** `R/pull_raw_occurrences.R`
-   **Example wrapper:** `scripts/pull_raw_species_set_6sp_test_v_2.0.R`
-   **Outputs (example):**
    -   `data/raw/gbif/gbif_<species_slug>_clean.csv`
    -   `data/raw/nbn/nbn_<species_slug>_clean.csv`

## Access and credentials

Different sources and query sizes can require different access steps:

-   **NBN Atlas (UK):** requires an account and login details (typically email/username + password).\
    These are used by `galah` to access and download data.

-   **GBIF (Europe):** can be retrieved in two ways:

    -   **Direct API pull** for smaller result sets (fast; typically used under \~100,000 rows).
    -   **GBIF download job** for larger result sets (asynchronous; may require GBIF credentials and can take time to complete).

The Stage 00 wrapper controls which method is used (e.g. `gbif_method = "auto"`) and whether to wait for large downloads to complete (e.g. `gbif_download_wait`).

## Caching behaviour (why there is a `use_cache` switch)

Stage 00 can re-use previously downloaded CSVs to avoid repeatedly querying GBIF/NBN.

-   If `use_cache = TRUE`, existing raw-clean CSVs are used when present.
-   Caching is **not blind**. If older cached files are missing newly-added QA/provenance fields (columns introduced in later revisions of the pull engine), they are treated as **stale** and will be re-pulled automatically when the upstream APIs are available.

This prevents the pipeline silently running on outdated raw outputs after the pull stage has been improved.

## Output fields retained

Stage 00 aims to retain useful provenance/metadata fields (not just coordinates and dates), because later stages (policy filtering and audit trails) may depend on them (e.g. record type, taxonomic rank, dataset key, licensing, GBIF issues).

The blocks below inspect one example GBIF raw-clean file and one example NBN raw-clean file (if present locally) and summarise the currently-retained column names.

```{r}
suppressPackageStartupMessages({
  library(readr)
  library(dplyr)
  library(stringr)
  library(tibble)
})

.read_header <- function(path) {
  if (is.null(path) || is.na(path) || !file.exists(path)) return(character())
  x <- readr::read_csv(path, n_max = 0, show_col_types = FALSE, progress = FALSE)
  names(x)
}

gbif_hits <- list.files("data/raw/gbif", pattern = "^gbif_.*_clean\\.csv$", recursive = TRUE, full.names = TRUE)
nbn_hits  <- list.files("data/raw/nbn",  pattern = "^nbn_.*_clean\\.csv$",  recursive = TRUE, full.names = TRUE)

gbif_example <- if (length(gbif_hits) > 0) gbif_hits[1] else NA_character_
nbn_example  <- if (length(nbn_hits)  > 0) nbn_hits[1]  else NA_character_

gbif_cols <- if (!is.na(gbif_example)) .read_header(gbif_example) else character()
nbn_cols  <- if (!is.na(nbn_example))  .read_header(nbn_example)  else character()

tibble(
  source = c("GBIF", "NBN"),
  example_file = c(gbif_example, nbn_example),
  n_columns = c(length(gbif_cols), length(nbn_cols))
)
```

```{r, results="asis"}
.cat_details <- function(title, cols) {
  if (length(cols) == 0) {
    cat("<p><em>", title, ":</em> no local example file found.</p>", sep = "")
    return(invisible(NULL))
  }
  cat("<details open><summary><strong>", title, " (click to collapse)</strong></summary>", sep = "")
  cat("<pre style='white-space: pre-wrap;'>", paste(cols, collapse = ", "), "</pre>", sep = "")
  cat("</details>")
  invisible(NULL)
}

.cat_details("GBIF columns retained", gbif_cols)
.cat_details("NBN columns retained",  nbn_cols)
```

## Notes on GBIF’s “Europe” filter

GBIF records are pulled using `hasCoordinate=TRUE` and `continent="EUROPE"`.

**Caveat:** GBIF’s `continent` is an interpreted field and may be blank when indeterminate (and seas are not assigned a continent). Filtering by continent can therefore exclude some records that are geographically in Europe but have no interpreted continent value.

If needed later, the pipeline can switch to explicit coordinate-based spatial filtering rather than relying on GBIF’s interpreted continent field.

------------------------------------------------------------------------

# Stage 01 — Merge

::: {.callout-note title="What this stage does"}
Stage 01 merges GBIF + NBN into a single table per species (retaining all original columns) and adds a small set of derived fields used downstream (IDs, parsed day, rounded coordinates, keys).

It then performs a very conservative cross-source duplicate removal step.
:::

-   **Engine script:** `R/merge_dedup_occurrences.R`
-   **Example wrapper:** `scripts/merge_dedup_species_set_6sp_test.R`
-   **Inputs:** `data/raw/gbif/...` and `data/raw/nbn/...`
-   **Outputs:** `data/processed/01_merged/<species_slug>/occ_<species_slug>__merged.(parquet|rds)`
-   **Run log:** `data/processed/01_merged/_runlog_01_merged.csv`

## Key choice: only the safest duplicates are dropped

This stage auto-drops only duplicates that meet all of the following:

-   exactly **1 GBIF** record and **1 NBN** record,
-   the same **rounded coordinates** (controlled by `coord_round_dp`),
-   both records have a true **day-level date** (YYYY-MM-DD; not a range/interval),
-   the non-preferred source is dropped (controlled by `prefer_source`, often GBIF).

Everything else remains in the merged output (no aggressive de-duplication at this stage).

------------------------------------------------------------------------

# Stage 02 — QC flags

::: {.callout-note title="What this stage does"}
Stage 02 adds QC flag columns to support later filtering and diagnostics.

No records are deleted here; it only annotates records with flags.
:::

-   **Engine script:** `R/qc_flag_occurrences.R`
-   **Example wrapper:** `scripts/qc_flag_species_set_6sp_test.R`
-   **Inputs:** `data/processed/01_merged/<species_slug>/occ_<species_slug>__merged.(parquet|rds)`
-   **Outputs:** `data/processed/02_qc_flagged/<species_slug>/occ_<species_slug>__qc_flagged.(parquet|rds)`
-   **Run log:** `data/processed/02_qc_flagged/_runlog_02_qc_flagged.csv`

## QC flags written

Stage 02 writes the following TRUE/FALSE flags:

-   `qc_flag_missing_coords` — lon or lat is missing\
-   `qc_flag_coords_out_of_range` — lon/lat outside valid numeric ranges\
-   `qc_flag_missing_date` — no day-level date and no year available\
-   `qc_flag_future_date` — day-level date in the future, or year in the future\
-   `qc_flag_uncertainty_high` — coordinate uncertainty exceeds a threshold\
-   `qc_flag_unexpected_licence` — `licence_expected` is FALSE (if present)\
-   `qc_flag_has_issues` — `issues` field is non-empty (GBIF-style issues list)\
-   `qc_flag_any` — TRUE if any of the above flags are TRUE

Optionally, it also writes:

-   `qc_flag_count` — number of TRUE flags per record

## Why flags (instead of filtering) are useful

-   QC problems remain visible and quantifiable (per species, per source).
-   Stage 03 policy becomes explicit and auditable (e.g. “we exclude future-dated records”).
-   Sensitivity tests become straightforward later (e.g. keep vs drop high uncertainty records).

------------------------------------------------------------------------

# Stage 03 — Policy filter

::: {.callout-note title="What this stage does"}
Stage 03 applies an explicit modelling policy (“in/out” definition) to decide which records are retained for modelling.

Stage 02 flags potential issues; Stage 03 is where records are removed according to agreed rules.
:::

-   **Engine script:** `R/filter_occurrences.R`
-   **Example wrapper:** `scripts/filter_species_set_6sp_test.R`
-   **Inputs:** `data/processed/02_qc_flagged/<species_slug>/occ_<species_slug>__qc_flagged.(parquet|rds)`
-   **Outputs:** `data/processed/03_filtered/<species_slug>/occ_<species_slug>__filtered.(parquet|rds)`
-   **Run log (recommended):** `data/processed/03_filtered/_runlog_03_filtered.csv`

## What gets decided here

Stage 03 is where the modelling definition is made concrete. Typical choices include:

-   **Time window:** what counts as “contemporary” (e.g. keep only records after a start date).
-   **Spatial reliability:** acceptable coordinate uncertainty (and what to do when uncertainty is missing).
-   **Record type / provenance:** whether to restrict to in-situ observations (especially for GBIF).
-   **Licensing and metadata:** whether to exclude records with unexpected licences.
-   **GBIF issues:** whether to ignore issues, drop any issues, or drop only specific issue codes.

These are policy choices rather than “data cleaning”. They are expected to change over time and be re-run.

## How the policy is expressed

The wrapper defines a single `policy <- list(...)` acting as a control panel. The `policy_id` should change whenever any policy threshold/switch changes, so outputs and run logs can be traced back to the rule set that produced them.

### Core policy switches supported by the engine

| Policy element | What it controls | Notes |
|------------------------|------------------------|------------------------|
| `drop_missing_coords` | drop rows with missing lon/lat | usually TRUE |
| `drop_coords_out_of_range` | drop impossible lon/lat values | usually TRUE |
| `drop_future_date` | drop future-dated records | usually TRUE |
| `drop_missing_date` | drop rows with no event day and no year | often FALSE |
| `min_date`, `max_date` | date window applied to `event_day` | set to NA to disable |
| `allow_year_only` | allow year-only records to satisfy date window | TRUE keeps more data |
| `require_event_day` | require day-level dates | strict; often FALSE |
| `min_year`, `max_year` | year window when using year-only fallback | optional |
| `max_coord_uncertainty_m` | drop if uncertainty exceeds threshold | set NA to disable |
| `uncertainty_missing_action` | keep or drop when uncertainty is missing | `"keep"` or `"drop"` |
| `gbif_issues_mode` | how to handle GBIF `issues` | `"ignore"`, `"drop_any"`, `"drop_blacklist"` |
| `issues_blacklist` | issue codes to drop when using blacklist mode | exact issue codes |
| `drop_unexpected_licence` | drop when `licence_expected == FALSE` | optional |
| `allowed_basis_of_record` / `drop_basis_of_record` | basis-of-record allow/deny lists | use with care for NBN |
| `allowed_taxon_rank` | allow only certain taxon ranks | optional |
| `nbn_certainty_col` / `nbn_allowed_certainty` | NBN-only certainty filtering | optional |
| `extra_drop_rules` | additional named rules (functions) | used for bespoke policy |

### “In-situ observations only” (GBIF) in the example wrapper

The example Stage 03 wrapper implements “in-situ observations only” for GBIF using an `extra_drop_rules` function. It keeps only GBIF rows where `basisOfRecord` is one of:

-   `HUMAN_OBSERVATION`, `OBSERVATION`, `MACHINE_OBSERVATION`

This is applied to GBIF only, so NBN rows are not unintentionally removed due to missing or different metadata.

## What gets logged (and why)

If `write_runlog = TRUE`, Stage 03 appends one row per species to:

-   `data/processed/03_filtered/_runlog_03_filtered.csv`

This run log includes:

-   `policy_id`, timestamps, species/slug, input file paths, output file paths
-   `n_in` and `n_out`
-   a set of `dropped_*` columns giving **drop counts per rule** (including `dropped_extra__...` rules)

This is the main audit trail for “what changed?” discussions, and supports reporting without opening the full datasets.

------------------------------------------------------------------------

# Stage 04 — Grid (to fill next)

::: {.callout-note title="What this stage will describe"}
Stage 04 converts filtered point records into gridded presence/background.

This includes choices such as grid resolution (e.g. 25 km), coordinate system, modelling extent (bounding box), and optional land masking.
:::

-   **Expected output root:** `data/processed/04_grid/`\
    (subfolders can capture the grid specification, e.g. `grid25km_test_landmask_europe_bbox`)

-   **Engine script:** (to fill)

-   **Wrapper script:** (to fill)

-   **Outputs:** (to fill)
