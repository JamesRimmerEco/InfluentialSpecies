---
title: "Influential species mapping - test full pull"
author: "James EV Rimmer"
date: "2026-01-15"
output:
  html_document:
    toc: true
    toc_depth: 3
---

### Introduction
This script is a full test of the start of a production workflow which builds on code tested in the "GBIF_NBN test" script.R" script. It pulls occurrence data for a single test species and applies basic screening so that tables of data extracted from **GBIF** and **NBN** can be later combined and ready to be imported to the Google Earth Engine in later steps. 

**Species test case:** *Vespula vulgaris* (common wasp)

**Goal (GBIF section):**
- demonstrate pulling data through the GBIF API (without an arbitrary record cap)
- apply basic screening: geography, coordinate presence, accepted licence set
- apply basic within-source de-duplication (lat/lon, ID)

**Accepted licences (GBIF records):**
- **CC0_1_0** – Public Domain Dedication
- **CC_BY_4_0** – Attribution 4.0
- **CC_BY_NC_4_0** – Attribution–NonCommercial 4.0

### Packages

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rgbif)
library(dplyr)
library(readr)
library(stringr)
library(lubridate)
```

### Settings

We set the species, geographic scope, and accepted licences for GBIF records.  
For GBIF downloads we  page through multiple API calls (batch pulls) so we can retrieve all records without an arbitrary record cap.

```{r}
species_name <- "Vespula vulgaris"      # scientific name
region_scope <- "EUROPE"               # GBIF continent filter
pause_s      <- 0.25                   # delay to reduce rate-limit risk

allowed_licences <- c("CC0_1_0", "CC_BY_4_0", "CC_BY_NC_4_0")  # adjust if needed

slug <- stringr::str_replace_all(tolower(species_name), "[^a-z0-9]+", "_") %>% 
  stringr::str_replace_all("^_+|_+$", "") # trim leading/trailing underscores
 # make a safe file-friendly species label (used in filenames)

# Output dirs (relative to scripts/)
gbif_out_dir <- file.path("..", "data", "raw", "gbif", "wasps")
ckpt_root    <- file.path("..", "data", "_checkpoints")  # shared checkpoint root (outside data/raw/)
gbif_ckpt_dir <- file.path(ckpt_root, "gbif")

# GBIF batch pull settings:
# - page_size controls the size of each API call (not a cap on total records)
# - max_records is a safety brake during development; set Inf for no cap
page_size   <- 1000
max_records <- Inf

```

## GBIF taxon resolution 
We first resolve the species name against the GBIF "backbone" so we can be sure we have a stable taxon identifier.

```{r}
# Purpose: resolve species name to a stable GBIF usageKey (avoid uncertain matching later)
bb <- name_backbone(name = species_name)

taxon_key <- bb$usageKey
message("GBIF match: ", bb$scientificName, " (usageKey=", taxon_key, ", matchType=", bb$matchType, ")")

# Display key info in the report
tibble(
  query = species_name,
  matched_name = bb$scientificName,
  usageKey = taxon_key,
  matchType = bb$matchType
)

```

## GBIF occurrence pull
We now pull all GBIF occurrence records for this taxon within the chosen geographic area.  
GBIF returns results in pages, so we iterate over pages until we have pulled the full set.

A small “record check” is included to confirm which licence strings are present in the raw pull before we filter.


```{r}
# Purpose: pull all GBIF occurrences for the species within the chosen scope (see settings above) 

# Checkpoint + retry settings (for long pulls)
dir.create(gbif_out_dir, recursive = TRUE, showWarnings = FALSE)
dir.create(gbif_ckpt_dir, recursive = TRUE, showWarnings = FALSE)

ckpt_file <- file.path(gbif_ckpt_dir, paste0("gbif_pull_checkpoint_", slug, ".rds"))
 # where we save progress for resuming

max_retries <- 5          # how many times to retry a failed page request before giving up
retry_base_wait_s <- 10   # base wait between retries; multiplies by attempt number (10s, 20s, 30s, ...)

start <- 0                     # record offset for paging through GBIF results (0 = start of result set)
all_pages <- list()            # container to store each page of results (combined at the end)
total_expected <- NA_integer_  # total number of matching records (read from GBIF meta on first successful call)

# If a checkpoint exists, resume instead of starting from scratch
if (file.exists(ckpt_file)) {
  ckpt <- readRDS(ckpt_file)            # load previously saved progress
  start <- ckpt$start                   # resume from the last recorded offset
  all_pages <- ckpt$all_pages           # keep pages already downloaded
  total_expected <- ckpt$total_expected # retain expected total for progress tracking

  message("Resuming GBIF pull from start = ", start,
          " (pages already stored: ", length(all_pages), ").")
}

repeat {
  Sys.sleep(pause_s) # small pause to avoid hammering the GBIF API

  # ---- Page request with retries (handles transient curl/network drops) ----
  res <- NULL
  for (attempt in seq_len(max_retries)) {
    res <- tryCatch(
      occ_search(
        taxonKey = taxon_key,        # stable GBIF identifier for the species
        continent = region_scope,    # restrict records to the chosen geographic scope
        hasCoordinate = TRUE,        # only return records with spatial coordinates (required for mapping + later dedup)
        limit = page_size,           # number of records per API call (page size; not a cap on total records)
        start = start                # offset for this page within the full result set
      ),
      error = function(e) e          # capture the error so we can retry instead of crashing immediately
    )

    if (!inherits(res, "error")) break  # success: exit retry loop and continue

    wait_s <- retry_base_wait_s * attempt # increasing backoff gives the API/network time to recover
    message("GBIF request failed at start = ", start,
            " (attempt ", attempt, "/", max_retries, "): ",
            conditionMessage(res),
            " | waiting ", wait_s, "s then retrying...")
    Sys.sleep(wait_s)
  }

  # If it still fails after retries, save progress and stop (so we can resume later)
  if (inherits(res, "error")) {
    saveRDS(list(start = start, all_pages = all_pages, total_expected = total_expected), ckpt_file)
    stop(res)
  }

  # Total number of records that match the query in GBIF (across all pages); captured once for progress tracking
  if (is.na(total_expected)) total_expected <- res$meta$count

  # Safety check: stop paging if the API returns an empty page (no records)
  if (length(res$data) == 0) break

  all_pages[[length(all_pages) + 1]] <- res$data  # append this page of records to our list of pages

  # Save progress after each successful page so we don't lose work if the session/connection dies
  saveRDS(list(start = start, all_pages = all_pages, total_expected = total_expected), ckpt_file)

  pulled_so_far <- start + nrow(res$data) # number pulled so far = start offset + rows returned in this page
  message("Pulled ", pulled_so_far, " / ", total_expected, " rows...")

  # Stop when we have everything (or hit the development safety brake)
  if (pulled_so_far >= total_expected) break
  if (pulled_so_far >= max_records) {
    message("Reached max_records safety limit (", max_records, ").")
    break
  }

  start <- start + nrow(res$data) # advance by what we actually received (avoids skipping if a page is short)
}

gbif_raw <- bind_rows(all_pages) # combine all pages into one data frame for downstream cleaning/screening

message("GBIF expected rows (query): ", total_expected)
message("GBIF rows pulled (paged):  ", nrow(gbif_raw))

```

## Record check (raw pull)

Before any screening, we check:
- the licence strings present in the raw pull (as GBIF supplies them)
- a quick glimpse of key fields, just to confirm the structure is as expected

```{r}
# Inspect what we pulled before filtering (including licence strings)

# Licence strings as supplied by GBIF (typically URLs)
gbif_raw %>%
  count(license, sort = TRUE) %>%
  mutate(prop = n / sum(n)) %>%
  print(n = Inf)

# Quick structure check on key fields
gbif_raw %>%
  select(gbifID, occurrenceID, decimalLongitude, decimalLatitude, eventDate, year, countryCode, license) %>%
  head(10)

```

## Basic screening + essential fields

GBIF supplies licence information as URLs. We modify these URLs to short recognisable codes and then filter to an accepted set (this might not filter in practice if all licenses are acceptable).   
We also keep only the essential fields we’ll need later (including for cross-source duplicate checks and GEE import).

```{r}
# Purpose: standardise essential fields and apply basic screens (coords + accepted licence set)

# Licence normaliser (GBIF uses URLs with 'licenses' in the path)
lic_normalise <- function(x) {
  dplyr::case_when(
    stringr::str_detect(x, "publicdomain/zero/1.0") ~ "CC0_1_0",
    stringr::str_detect(x, "licenses/by/4.0") ~ "CC_BY_4_0",
    stringr::str_detect(x, "licenses/by-nc/4.0") ~ "CC_BY_NC_4_0",
    TRUE ~ NA_character_
  )
}

gbif_clean <- gbif_raw %>%
  transmute(
    source = "GBIF",
    species = species_name,
    gbifID = gbifID,
    occurrenceID = occurrenceID,
    lon = decimalLongitude,
    lat = decimalLatitude,
    date = eventDate,
    year = year,
    country = countryCode,
    licence_raw = license,
    licence = lic_normalise(license)
  ) %>%
  filter(!is.na(lon), !is.na(lat)) %>%
  filter(!is.na(licence) & licence %in% allowed_licences)

message("GBIF rows (raw -> screened): ", nrow(gbif_raw), " -> ", nrow(gbif_clean))

# Quick summaries
gbif_clean %>% count(licence, sort = TRUE)
gbif_clean %>% count(country, sort = TRUE) %>% head(10)


```

## De-duplication

We remove obvious within-source duplicates:
1) duplicate `gbifID` (the same GBIF record repeated)
2) identical `(lon, lat, date)` combinations (a light redundancy screen)

This helps keep the table tidy before we later combine GBIF with NBN and screen cross-source duplicates.

```{r}
# Purpose: light within-source de-duplication before later cross-source merging

n_before <- nrow(gbif_clean)

gbif_clean <- gbif_clean %>%
  distinct(gbifID, .keep_all = TRUE) %>%
  distinct(lon, lat, date, .keep_all = TRUE)

message("GBIF rows (screened -> de-dup): ", n_before, " -> ", nrow(gbif_clean))

```
## Save output

We save the screened, de-duplicated GBIF table so we can reuse it without re-pulling from the API.

```{r}
# Purpose: save clean GBIF table for reuse
gbif_outfile <- file.path(gbif_out_dir, paste0("gbif_", slug, "_clean.csv"))

write_csv(gbif_clean, gbif_outfile)
gbif_outfile

```


## GBIF summary

At this point we have:
- resolved *Vespula vulgaris* to a stable GBIF identifier
- pulled all GBIF occurrence records in the European scope
- checked raw licence strings and record structure before screening
- standardised essential fields, filtered to accepted licences and records with coordinates
- applied light de-duplication by coordinates and ID
- optionally saved a clean CSV for reuse

# ============================================================
# NBN (UK) occurrence pull (galah / UK atlas)

```{r}
library(galah)

# NBN access: use the email associated with your atlas account
nbn_email <- "jamesrimmer92@mail.com"

# Accepted licences (NBN tends to return short codes like CC-BY, CC0, etc.)
allowed_licences_nbn <- c("OGL", "CC0", "CC-BY", "CC-BY-NC")

# Output dirs (relative to scripts/)
nbn_out_dir  <- file.path("..", "data", "raw", "nbn", "wasps")
nbn_ckpt_dir <- file.path(ckpt_root, "nbn")

# Configure galah to use the UK atlas + provide a download reason
galah_config(atlas = "United Kingdom", email = nbn_email, verbose = FALSE)
galah_config(download_reason_id = 17) # 17 = research/academic purposes (as per your earlier working setup)
```

## NBN taxon check

We first confirm that the species name resolves cleanly in the UK atlas.

```{r}
nbn_taxa <- search_taxa(species_name)

message("NBN taxon search (top hit):")
nbn_taxa %>%
  dplyr::select(scientific_name, taxon_concept_id, rank) %>%
  head(1)
```

## NBN occurrence pull

We now pull all occurrences for the species from the UK atlas.  
(We keep the field selection minimal and consistent with GBIF so later merging is straightforward.)

```{r}
dir.create(nbn_out_dir,  recursive = TRUE, showWarnings = FALSE)
dir.create(nbn_ckpt_dir, recursive = TRUE, showWarnings = FALSE)

nbn_outfile <- file.path(nbn_out_dir, paste0("nbn_", slug, "_clean.csv"))

# Checkpoint file for NBN raw pull (so we can resume without re-downloading)
nbn_ckpt_file <- file.path(nbn_ckpt_dir, paste0("nbn_pull_checkpoint_", slug, ".rds"))

max_retries <- 5         # how many times to retry a failed request before giving up
retry_base_wait_s <- 10  # base wait between retries; multiplies by attempt number (10s, 20s, 30s, ...)

# If we've already saved a clean file, reuse it (avoids re-pulling from the API)
if (file.exists(nbn_outfile)) {
  message("Found existing NBN clean file, reading: ", nbn_outfile)
  nbn_clean <- readr::read_csv(nbn_outfile, show_col_types = FALSE)

} else {

  # If a checkpoint exists, reuse the raw pull from disk (avoid re-downloading)
  if (file.exists(nbn_ckpt_file)) {
    message("Found NBN checkpoint, loading: ", nbn_ckpt_file)
    nbn_raw <- readRDS(nbn_ckpt_file)

  } else {

    # Pull raw occurrences (with simple retries for transient network/API issues)
    nbn_raw <- NULL
    for (attempt in seq_len(max_retries)) {

      nbn_raw <- tryCatch(
        galah_call() |>
          galah_identify(species_name) |>
          atlas_occurrences(
            select = galah_select(
              recordID,
              scientificName,
              eventDate,
              year,
              decimalLatitude,
              decimalLongitude,
              license  # request field as 'license' (returned column name is usually `dcterms:license`)
            )
          ),
        error = function(e) e
      )

      if (!inherits(nbn_raw, "error")) break  # success

      wait_s <- retry_base_wait_s * attempt
      message("NBN request failed (attempt ", attempt, "/", max_retries, "): ",
              conditionMessage(nbn_raw),
              " | waiting ", wait_s, "s then retrying...")
      Sys.sleep(wait_s)
    }

    # If it still fails after retries, stop
    if (inherits(nbn_raw, "error")) stop(nbn_raw)

    # Save raw pull to checkpoint so we don't need to re-download next time
    saveRDS(nbn_raw, nbn_ckpt_file)
    message("Saved NBN checkpoint: ", nbn_ckpt_file)
  }

  message("NBN raw rows: ", nrow(nbn_raw))
}

```

## Record check + cleaning (NBN)

```{r}
# Only run if we have raw NBN data (i.e. we didn't already load a clean CSV)
if (exists("nbn_raw") && !exists("nbn_clean")) {

  lic_col <- if ("dcterms:license" %in% names(nbn_raw)) "dcterms:license" else "license"

  # Record check: licence strings as returned by NBN Atlas
  message("NBN licence breakdown (RAW pull):")
  nbn_raw %>%
    dplyr::count(.data[[lic_col]], sort = TRUE) %>%
    dplyr::mutate(prop = n / sum(n)) %>%
    print(n = Inf)

  # Normalise NBN licence strings to short codes
  lic_normalise_nbn <- function(x) {
    x_l <- stringr::str_to_lower(x)

    dplyr::case_when(
      is.na(x) ~ NA_character_,
      x %in% c("OGL", "CC0", "CC-BY", "CC-BY-NC") ~ x,
      stringr::str_detect(x_l, "open government licence|\\bogl\\b") ~ "OGL",
      stringr::str_detect(x_l, "\\bcc0\\b|publicdomain/zero/1.0") ~ "CC0",
      stringr::str_detect(x_l, "cc[- ]?by[- ]?nc|licenses/by-nc/4.0") ~ "CC-BY-NC",
      stringr::str_detect(x_l, "cc[- ]?by\\b|licenses/by/4.0") ~ "CC-BY",
      TRUE ~ NA_character_
    )
  }

  # Create nbn_clean in the same column style as gbif_clean
  nbn_clean <- nbn_raw %>%
    dplyr::transmute(
      source = "NBN",
      species = species_name,
      recordID = recordID,
      lon = decimalLongitude,
      lat = decimalLatitude,
      date = eventDate,
      year = year,
      licence_raw = .data[[lic_col]],
      licence = lic_normalise_nbn(.data[[lic_col]])
    ) %>%
    dplyr::filter(!is.na(lon), !is.na(lat)) %>%
    dplyr::filter(!is.na(licence) & licence %in% allowed_licences_nbn)
  
  message("NBN rows (raw -> screened): ", nrow(nbn_raw), " -> ", nrow(nbn_clean))
}
```

## De-duplication (NBN)

As with GBIF, we apply de-duplication to remove identical reports by ID or by co-ordinate and date.

```{r}
if (exists("nbn_clean")) {
  n_before <- nrow(nbn_clean)

  nbn_clean <- nbn_clean %>%
    dplyr::distinct(recordID, .keep_all = TRUE) %>%      # remove exact duplicates by record ID
    dplyr::distinct(lon, lat, date, .keep_all = TRUE)    # remove identical lon/lat/date duplicates

  message("NBN rows (screened -> de-dup): ", n_before, " -> ", nrow(nbn_clean))
}
```

## Save output (NBN)

Save a clean NBN CSV alongside the GBIF outputs.

```{r nbn_save_output}
if (exists("nbn_clean")) {
  write_csv(nbn_clean, nbn_outfile)
  nbn_outfile
}
```

## NBN summary

At this point we have:
- resolved *Vespula vulgaris* in the UK atlas
- pulled NBN occurrence records
- checked raw licence strings before screening
- standardised essential fields, filtered to accepted licences and usable coordinates
- applied light de-duplication by coordinates + date, and ID
- saved a clean CSV for reuse





