---
title: "Influential species mapping - test full pull"
author: "James EV Rimmer"
date: "2026-01-15"
output:
  html_document:
    toc: true
    toc_depth: 3
---

### Introduction
This script is a full test of the start of a production workflow which builds on code tested in the "GBIF_NBN test" script.R" script. It pulls occurrence data for a single test species and applies basic screening so that tables of data extracted from **GBIF** and **NBN** can be later combined and ready to be imported to the Google Earth Engine in later steps. 

**Species test case:** *Vespula vulgaris* (common wasp)

**Goal (GBIF section):**
- demonstrate pulling data through the GBIF API (without an arbitrary record cap)
- apply basic screening: geography, coordinate presence, accepted licence set
- apply basic within-source de-duplication (lat/lon, ID)

**Accepted licences (GBIF records):**
- **CC0_1_0** – Public Domain Dedication
- **CC_BY_4_0** – Attribution 4.0
- **CC_BY_NC_4_0** – Attribution–NonCommercial 4.0

### Packages

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rgbif)
library(dplyr)
library(readr)
library(stringr)
library(lubridate)
```

### Settings

We set the species, geographic scope, and accepted licences for GBIF records.  
For GBIF downloads we  page through multiple API calls (batch pulls) so we can retrieve all records without an arbitrary record cap.

```{r}
species_name <- "Vespula vulgaris"      # scientific name
region_scope <- "EUROPE"               # GBIF continent filter
pause_s      <- 0.25                   # delay to reduce rate-limit risk

allowed_licences <- c("CC0_1_0", "CC_BY_4_0", "CC_BY_NC_4_0")  # adjust if needed

slug <- stringr::str_replace_all(tolower(species_name), "[^a-z0-9]+", "_") %>% 
  stringr::str_replace_all("^_+|_+$", "") # trim leading/trailing underscores
 # make a safe file-friendly species label (used in filenames)

# Output dirs (relative to scripts/)
gbif_out_dir <- file.path("..", "data", "raw", "gbif", "wasps")
ckpt_root    <- file.path("..", "data", "_checkpoints")  # shared checkpoint root (outside data/raw/)
gbif_ckpt_dir <- file.path(ckpt_root, "gbif")

# GBIF batch pull settings:
# - page_size controls the size of each API call (not a cap on total records)
# - max_records is a safety brake during development; set Inf for no cap
page_size   <- 1000
max_records <- Inf

```

## GBIF taxon resolution 
We first resolve the species name against the GBIF "backbone" so we can be sure we have a stable taxon identifier.

```{r}
# Purpose: resolve species name to a stable GBIF usageKey (avoid uncertain matching later)
bb <- name_backbone(name = species_name)

taxon_key <- bb$usageKey
message("GBIF match: ", bb$scientificName, " (usageKey=", taxon_key, ", matchType=", bb$matchType, ")")

# Display key info in the report
tibble(
  query = species_name,
  matched_name = bb$scientificName,
  usageKey = taxon_key,
  matchType = bb$matchType
)

```

## GBIF occurrence pull
We now pull all GBIF occurrence records for this taxon within the chosen geographic area.  
GBIF returns results in pages, so we iterate over pages until we have pulled the full set.

A small “record check” is included to confirm which licence strings are present in the raw pull before we filter.


```{r}
# Purpose: pull all GBIF occurrences for the species within the chosen scope (see settings above) 

# Checkpoint + retry settings (for long pulls)
dir.create(gbif_out_dir, recursive = TRUE, showWarnings = FALSE)
dir.create(gbif_ckpt_dir, recursive = TRUE, showWarnings = FALSE)

# Clean output file (used as a cache when knitting)
gbif_outfile <- file.path(gbif_out_dir, paste0("gbif_", slug, "_clean.csv"))

# If the clean file already exists, load it and skip the API pull
if (file.exists(gbif_outfile)) {

  message("Found existing GBIF clean file; skipping API pull: ", gbif_outfile)
  gbif_clean <- readr::read_csv(gbif_outfile, show_col_types = FALSE)

} else {

  ckpt_file <- file.path(gbif_ckpt_dir, paste0("gbif_pull_checkpoint_", slug, ".rds"))
  # where we save progress for resuming

  max_retries <- 5
  retry_base_wait_s <- 10

  start <- 0
  all_pages <- list()
  total_expected <- NA_integer_

  if (file.exists(ckpt_file)) {
    ckpt <- readRDS(ckpt_file)
    start <- ckpt$start
    all_pages <- ckpt$all_pages
    total_expected <- ckpt$total_expected

    message("Resuming GBIF pull from start = ", start,
            " (pages already stored: ", length(all_pages), ").")
  }

  repeat {
    Sys.sleep(pause_s)

    res <- NULL
    for (attempt in seq_len(max_retries)) {
      res <- tryCatch(
        occ_search(
          taxonKey = taxon_key,
          continent = region_scope,
          hasCoordinate = TRUE,
          limit = page_size,
          start = start
        ),
        error = function(e) e
      )

      if (!inherits(res, "error")) break

      wait_s <- retry_base_wait_s * attempt
      message("GBIF request failed at start = ", start,
              " (attempt ", attempt, "/", max_retries, "): ",
              conditionMessage(res),
              " | waiting ", wait_s, "s then retrying...")
      Sys.sleep(wait_s)
    }

    if (inherits(res, "error")) {
      saveRDS(list(start = start, all_pages = all_pages, total_expected = total_expected), ckpt_file)
      stop(res)
    }

    if (is.na(total_expected)) total_expected <- res$meta$count
    if (length(res$data) == 0) break

    all_pages[[length(all_pages) + 1]] <- res$data
    saveRDS(list(start = start, all_pages = all_pages, total_expected = total_expected), ckpt_file)

    pulled_so_far <- start + nrow(res$data)
    message("Pulled ", pulled_so_far, " / ", total_expected, " rows...")

    if (pulled_so_far >= total_expected) break
    if (pulled_so_far >= max_records) {
      message("Reached max_records safety limit (", max_records, ").")
      break
    }

    start <- start + nrow(res$data)
  }

  gbif_raw <- bind_rows(all_pages)

  message("GBIF expected rows (query): ", total_expected)
  message("GBIF rows pulled (paged):  ", nrow(gbif_raw))
}
```

## Record check (raw pull)

Before any screening, we check:
- the licence strings present in the raw pull (as GBIF supplies them)
- a quick glimpse of key fields, just to confirm the structure is as expected

```{r}
# Inspect what we pulled before filtering (including licence strings)

# Licence strings as supplied by GBIF (typically URLs)
if (exists("gbif_raw")) {

  gbif_raw %>%
    count(license, sort = TRUE) %>%
    mutate(prop = n / sum(n)) %>%
    print(n = Inf)

  gbif_raw %>%
    select(gbifID, occurrenceID, decimalLongitude, decimalLatitude, eventDate, year, countryCode, license) %>%
    head(10)

} else {

  message("GBIF raw pull not present in this session (loaded cached clean CSV instead).")

  gbif_clean %>%
    count(licence_raw, sort = TRUE) %>%
    mutate(prop = n / sum(n)) %>%
    print(n = Inf)

  gbif_clean %>%
    select(gbifID, occurrenceID, lon, lat, date, year, country, licence_raw, licence) %>%
    head(10)
}

```

## Basic screening + essential fields

GBIF supplies licence information as URLs. We modify these URLs to short recognisable codes and then filter to an accepted set (this might not filter in practice if all licenses are acceptable).   
We also keep only the essential fields we’ll need later (including for cross-source duplicate checks and GEE import).

```{r}
# Purpose: standardise essential fields and apply basic screens (coords + accepted licence set)

# Licence normaliser (GBIF uses URLs with 'licenses' in the path)
lic_normalise <- function(x) {
  dplyr::case_when(
    stringr::str_detect(x, "publicdomain/zero/1.0") ~ "CC0_1_0",
    stringr::str_detect(x, "licenses/by/4.0") ~ "CC_BY_4_0",
    stringr::str_detect(x, "licenses/by-nc/4.0") ~ "CC_BY_NC_4_0",
    TRUE ~ NA_character_
  )
}

if (exists("gbif_raw")) {

  gbif_clean <- gbif_raw %>%
    transmute(
      source = "GBIF",
      species = species_name,
      gbifID = gbifID,
      occurrenceID = occurrenceID,
      lon = decimalLongitude,
      lat = decimalLatitude,
      date = eventDate,
      year = year,
      country = countryCode,
      licence_raw = license,
      licence = lic_normalise(license)
    ) %>%
    filter(!is.na(lon), !is.na(lat)) %>%
    filter(!is.na(licence) & licence %in% allowed_licences)

  message("GBIF rows (raw -> screened): ", nrow(gbif_raw), " -> ", nrow(gbif_clean))

} else {

  message("Skipping GBIF screening (already loaded screened/de-duplicated gbif_clean from CSV).")
}

# Quick summaries
gbif_clean %>% count(licence, sort = TRUE)
gbif_clean %>% count(country, sort = TRUE) %>% head(10)

```

## De-duplication

We remove obvious within-source duplicates:
1) duplicate `gbifID` (the same GBIF record repeated)
2) identical `(lon, lat, date)` combinations (a light redundancy screen)

This helps keep the table tidy before we later combine GBIF with NBN and screen cross-source duplicates.

```{r}
# Purpose: light within-source de-duplication before later cross-source merging

if (exists("gbif_raw")) {

  n_before <- nrow(gbif_clean)

  gbif_clean <- gbif_clean %>%
    distinct(gbifID, .keep_all = TRUE) %>%
    distinct(lon, lat, date, .keep_all = TRUE)

  message("GBIF rows (screened -> de-dup): ", n_before, " -> ", nrow(gbif_clean))

} else {

  message("Skipping GBIF de-dup (cached gbif_clean already reflects previous de-duplication).")
}

```
## Save output

We save the screened, de-duplicated GBIF table so we can reuse it without re-pulling from the API.

```{r}
# Purpose: save clean GBIF table for reuse
gbif_outfile <- file.path(gbif_out_dir, paste0("gbif_", slug, "_clean.csv"))

if (!file.exists(gbif_outfile)) {
  write_csv(gbif_clean, gbif_outfile)
  message("Saved GBIF clean file: ", gbif_outfile)
} else {
  message("GBIF clean file already exists (left unchanged): ", gbif_outfile)
}

gbif_outfile

```


## GBIF summary

At this point we have:
- resolved *Vespula vulgaris* to a stable GBIF identifier
- pulled all GBIF occurrence records in the European scope
- checked raw licence strings and record structure before screening
- standardised essential fields, filtered to accepted licences and records with coordinates
- applied light de-duplication by coordinates and ID
- optionally saved a clean CSV for reuse

# ============================================================
# NBN (UK) occurrence pull (galah / UK atlas)

```{r}
library(galah)

# NBN access: use the email associated with my atlas account
nbn_email <- "jamesrimmer92@mail.com"

# Accepted licences (NBN tends to return short codes like CC-BY, CC0, etc.)
allowed_licences_nbn <- c("OGL", "CC0", "CC-BY", "CC-BY-NC")

# Output dirs (relative to scripts/)
nbn_out_dir  <- file.path("..", "data", "raw", "nbn", "wasps")
nbn_ckpt_dir <- file.path(ckpt_root, "nbn")

# Configure galah to use the UK atlas + provide a download reason
galah_config(atlas = "United Kingdom", email = nbn_email, verbose = FALSE)
galah_config(download_reason_id = 17) # 17 = research/academic purposes
```

## NBN taxon check

We first confirm that the species name resolves cleanly in the UK atlas.

```{r}
nbn_taxa <- search_taxa(species_name)

message("NBN taxon search (top hit):")
nbn_taxa %>%
  dplyr::select(scientific_name, taxon_concept_id, rank) %>%
  head(1)
```

## NBN occurrence pull

We now pull all occurrences for the species from the UK atlas.  
(We keep the field selection minimal and consistent with GBIF so later merging is straightforward.)

```{r}
dir.create(nbn_out_dir,  recursive = TRUE, showWarnings = FALSE)
dir.create(nbn_ckpt_dir, recursive = TRUE, showWarnings = FALSE)

nbn_outfile <- file.path(nbn_out_dir, paste0("nbn_", slug, "_clean.csv"))

# Checkpoint file for NBN raw pull (so we can resume without re-downloading)
nbn_ckpt_file <- file.path(nbn_ckpt_dir, paste0("nbn_pull_checkpoint_", slug, ".rds"))

max_retries <- 5         # how many times to retry a failed request before giving up
retry_base_wait_s <- 10  # base wait between retries; multiplies by attempt number (10s, 20s, 30s, ...)

# If we've already saved a clean file, reuse it (avoids re-pulling from the API)
if (file.exists(nbn_outfile)) {
  message("Found existing NBN clean file, reading: ", nbn_outfile)
  nbn_clean <- readr::read_csv(nbn_outfile, show_col_types = FALSE)

} else {

  # If a checkpoint exists, reuse the raw pull from disk (avoid re-downloading)
  if (file.exists(nbn_ckpt_file)) {
    message("Found NBN checkpoint, loading: ", nbn_ckpt_file)
    nbn_raw <- readRDS(nbn_ckpt_file)

  } else {

    # Pull raw occurrences (with simple retries for transient network/API issues)
    nbn_raw <- NULL
    for (attempt in seq_len(max_retries)) {

      nbn_raw <- tryCatch(
        galah_call() |>
          galah_identify(species_name) |>
          atlas_occurrences(
            select = galah_select(
              recordID,
              scientificName,
              eventDate,
              year,
              decimalLatitude,
              decimalLongitude,
              license  # request field as 'license' (returned column name is usually `dcterms:license`)
            )
          ),
        error = function(e) e
      )

      if (!inherits(nbn_raw, "error")) break  # success

      wait_s <- retry_base_wait_s * attempt
      message("NBN request failed (attempt ", attempt, "/", max_retries, "): ",
              conditionMessage(nbn_raw),
              " | waiting ", wait_s, "s then retrying...")
      Sys.sleep(wait_s)
    }

    # If it still fails after retries, stop
    if (inherits(nbn_raw, "error")) stop(nbn_raw)

    # Save raw pull to checkpoint so we don't need to re-download next time
    saveRDS(nbn_raw, nbn_ckpt_file)
    message("Saved NBN checkpoint: ", nbn_ckpt_file)
  }

  message("NBN raw rows: ", nrow(nbn_raw))
}

```

## Record check + cleaning (NBN)

```{r}
# Only run if we have raw NBN data (i.e. we didn't already load a clean CSV)
if (exists("nbn_raw") && !exists("nbn_clean")) {

  lic_col <- if ("dcterms:license" %in% names(nbn_raw)) "dcterms:license" else "license"

  # Record check: licence strings as returned by NBN Atlas
  message("NBN licence breakdown (RAW pull):")
  nbn_raw %>%
    dplyr::count(.data[[lic_col]], sort = TRUE) %>%
    dplyr::mutate(prop = n / sum(n)) %>%
    print(n = Inf)

  # Normalise NBN licence strings to short codes
  lic_normalise_nbn <- function(x) {
    x_l <- stringr::str_to_lower(x)

    dplyr::case_when(
      is.na(x) ~ NA_character_,
      x %in% c("OGL", "CC0", "CC-BY", "CC-BY-NC") ~ x,
      stringr::str_detect(x_l, "open government licence|\\bogl\\b") ~ "OGL",
      stringr::str_detect(x_l, "\\bcc0\\b|publicdomain/zero/1.0") ~ "CC0",
      stringr::str_detect(x_l, "cc[- ]?by[- ]?nc|licenses/by-nc/4.0") ~ "CC-BY-NC",
      stringr::str_detect(x_l, "cc[- ]?by\\b|licenses/by/4.0") ~ "CC-BY",
      TRUE ~ NA_character_
    )
  }

  # Create nbn_clean in the same column style as gbif_clean
  nbn_clean <- nbn_raw %>%
    dplyr::transmute(
      source = "NBN",
      species = species_name,
      recordID = recordID,
      lon = decimalLongitude,
      lat = decimalLatitude,
      date = eventDate,
      year = year,
      licence_raw = .data[[lic_col]],
      licence = lic_normalise_nbn(.data[[lic_col]])
    ) %>%
    dplyr::filter(!is.na(lon), !is.na(lat)) %>%
    dplyr::filter(!is.na(licence) & licence %in% allowed_licences_nbn)
  
  message("NBN rows (raw -> screened): ", nrow(nbn_raw), " -> ", nrow(nbn_clean))
}
```

## De-duplication (NBN)

As with GBIF, we apply de-duplication to remove identical reports by ID or by co-ordinate and date.

```{r}
if (exists("nbn_clean")) {
  n_before <- nrow(nbn_clean)

  nbn_clean <- nbn_clean %>%
    dplyr::distinct(recordID, .keep_all = TRUE) %>%      # remove exact duplicates by record ID
    dplyr::distinct(lon, lat, date, .keep_all = TRUE)    # remove identical lon/lat/date duplicates

  message("NBN rows (screened -> de-dup): ", n_before, " -> ", nrow(nbn_clean))
}
```

## Save output (NBN)

Save a clean NBN CSV alongside the GBIF outputs.

```{r nbn_save_output}
if (exists("nbn_clean")) {
  write_csv(nbn_clean, nbn_outfile)
  nbn_outfile
}
```

## NBN summary

At this point we have:
- resolved *Vespula vulgaris* in the UK atlas
- pulled NBN occurrence records
- checked raw licence strings before screening
- standardised essential fields, filtered to accepted licences and usable coordinates
- applied light de-duplication by coordinates + date, and ID
- saved a clean CSV for reuse

# ============================================================
## Merge GBIF + NBN and check cross-source duplicates

At this point we have two screened and within-source de-duplicated tables:

- `gbif_clean` (GBIF, Europe-wide)
- `nbn_clean` (NBN Atlas, UK-only)

There is no shared record ID between GBIF and NBN, so cross-source duplicate checking has to be approximate.
For this test we use:

- `species`
- *rounded coordinates* (to reduce tiny numerical differences between sources)
- `date`, when a full calendar day is available

We only automatically remove duplicates where the match is very clear (a simple one-to-one match between sources). Anything more complicated is kept and flagged for review.

### Reloading clean outputs (if data are already there)

Run the chunk below if on a fresh device (ensure the data are already in the correct folders)

```{r}
gbif_outfile <- file.path(gbif_out_dir, paste0("gbif_", slug, "_clean.csv"))
nbn_outfile  <- file.path(nbn_out_dir,  paste0("nbn_",  slug, "_clean.csv"))

if (!file.exists(gbif_outfile)) {
  stop("Can't find GBIF clean file at: ", gbif_outfile,
       "\nHave you run the GBIF section and saved the output?")
}
if (!file.exists(nbn_outfile)) {
  stop("Can't find NBN clean file at: ", nbn_outfile,
       "\nHave you run the NBN section and saved the output?")
}

gbif_clean <- readr::read_csv(gbif_outfile, show_col_types = FALSE)
nbn_clean  <- readr::read_csv(nbn_outfile,  show_col_types = FALSE)

message("Reloaded:")
message(" - GBIF: ", gbif_outfile, " (", nrow(gbif_clean), " rows)")
message(" - NBN : ", nbn_outfile,  " (", nrow(nbn_clean),  " rows)")
```

### Checking for duplicates 

**What we use as a matching key**
- **Species name** (here, one test species)
- **Location** (lon/lat, rounded to a fixed number of decimal places)
- **Time**
  - *Strict:* same **day** (only when we can safely extract a day from the date string)
  - *Loose:* same **year** (used only as a flag, not an automatic de-duplicate)

**Why it’s approximate**
- Coordinates can be rounded differently between sources, or have small jitter.
- GBIF dates are not always a clean single day (they can be timestamps, ranges, or missing).
- Two different observers could genuinely record the same species at the same location on the same day. If we collapse those, we might be losing true separate records.

**Safety rule**
- We only remove cross-source duplicates automatically when the match is a simple one-to-one pair (exactly one GBIF record and one NBN record for the same strict day-level key).
- Anything messier than that is kept, but clearly flagged and printed so I can sanity check it. 

```{r}
# Purpose: stop early if the expected cleaned tables are not present (run section above first and earlier)
if (!exists("gbif_clean")) stop("gbif_clean not found - run the GBIF section first.")
if (!exists("nbn_clean")) stop("nbn_clean not found - run the NBN section first.")
```

## Standardise both tables into a common shape

We standardise column names so GBIF and NBN can be bound row-wise cleanly, while still keeping traceability back to each source.

```{r merge_prep, echo=FALSE, message=FALSE, warning=FALSE, results="hide"}
# ---- Merge + cross-source duplicate checks (tidy outputs) ----

# Choose which source "wins" when we have a clean 1-to-1 match on the same day + location.
# (Keeping GBIF by default, because it already covers the full Europe scope.)
prefer_source <- "GBIF"  # or "NBN"

# 1) Standardise columns so GBIF + NBN can be row-bound cleanly
occ_gbif <- gbif_clean %>%
  transmute(
    source,
    species,
    record_id    = as.character(gbifID),      # <- force to character
    occurrenceID = as.character(occurrenceID),
    lon, lat,
    date_raw     = as.character(date),
    year         = as.integer(year),
    country      = as.character(country),
    licence_raw  = as.character(licence_raw),
    licence      = as.character(licence)
  )

occ_nbn <- nbn_clean %>%
  transmute(
    source,
    species,
    record_id    = as.character(recordID),    
    occurrenceID = NA_character_,
    lon, lat,
    date_raw     = as.character(date),
    year         = as.integer(year),
    country      = "GB",
    licence_raw  = as.character(licence_raw),
    licence      = as.character(licence)
  )

occ_merged_all <- bind_rows(occ_gbif, occ_nbn)

# 2) Parse date precision + build match keys
#    - "day" precision: yyyy-mm-dd parseable (not a range)
#    - "year" precision: year exists but no clean day
#    - "unknown": neither is reliable
occ_merged_all <- occ_merged_all %>%
  mutate(
    date_is_range = !is.na(date_raw) & stringr::str_detect(date_raw, "/"),
    date_day_str  = dplyr::if_else(!is.na(date_raw), stringr::str_sub(date_raw, 1, 10), NA_character_),
    date_day      = suppressWarnings(lubridate::ymd(date_day_str)),
    date_precision = dplyr::case_when(
      !is.na(date_day) & !date_is_range ~ "day",
      !is.na(year) ~ "year",
      TRUE ~ "unknown"
    ),

    # coordinate keys (4 dp ~ ~11 m; conservative but usually fine for this purpose)
    lon_key = round(lon, 4),
    lat_key = round(lat, 4),

    # strict day key: species + rounded coords + exact day
    key_day = dplyr::if_else(
      date_precision == "day",
      paste0(species, "|", sprintf("%.4f", lon_key), "|", sprintf("%.4f", lat_key), "|", as.character(date_day)),
      NA_character_
    ),

    # loose year key: species + rounded coords + year (used only for flagging)
    key_year = dplyr::if_else(
      !is.na(year),
      paste0(species, "|", sprintf("%.4f", lon_key), "|", sprintf("%.4f", lat_key), "|", year),
      NA_character_
    )
  )
```


```{r merge_logic, echo=FALSE, message=FALSE, warning=FALSE, results="hide"}
# 3) Flag potential cross-source duplicates
day_key_summary <- occ_merged_all %>%
  filter(!is.na(key_day)) %>%
  group_by(key_day) %>%
  summarise(
    n_total = n(),
    n_gbif  = sum(source == "GBIF"),
    n_nbn   = sum(source == "NBN"),
    .groups = "drop"
  ) %>%
  mutate(has_both_sources = (n_gbif > 0 & n_nbn > 0))

keys_day_both <- day_key_summary %>%
  filter(has_both_sources) %>%
  pull(key_day)

# Year-level "loose" flags: only when the year key matches across sources AND at least one record lacks day precision
year_key_summary <- occ_merged_all %>%
  filter(!is.na(key_year)) %>%
  group_by(key_year) %>%
  summarise(
    n_total = n(),
    n_gbif  = sum(source == "GBIF"),
    n_nbn   = sum(source == "NBN"),
    n_not_day = sum(date_precision != "day"),
    .groups = "drop"
  ) %>%
  mutate(flag_loose = (n_gbif > 0 & n_nbn > 0 & n_not_day > 0))

keys_year_loose <- year_key_summary %>%
  filter(flag_loose) %>%
  pull(key_year)

occ_merged_all <- occ_merged_all %>%
  mutate(
    cross_dup_day        = !is.na(key_day)  & key_day  %in% keys_day_both,
    cross_dup_year_loose = !is.na(key_year) & key_year %in% keys_year_loose
  )

# 4) Decide what we drop (conservative rule)
#    We only auto-drop 1-to-1 day matches (exactly one GBIF + one NBN record in the same key_day).
one_to_one_day_keys <- day_key_summary %>%
  filter(has_both_sources, n_total == 2, n_gbif == 1, n_nbn == 1) %>%
  pull(key_day)

dropped_strict <- occ_merged_all %>%
  filter(key_day %in% one_to_one_day_keys, source != prefer_source) %>%
  mutate(drop_reason = paste0("Strict 1-to-1 day match; kept ", prefer_source))

occ_merged_dedup_strict <- occ_merged_all %>%
  filter(!(key_day %in% one_to_one_day_keys & source != prefer_source))

# 5) A “review” table for anything more complicated than 1-to-1
occ_crossdup_review <- occ_merged_all %>%
  filter(cross_dup_day) %>%
  group_by(key_day) %>%
  filter(!(n() == 2 & sum(source == "GBIF") == 1 & sum(source == "NBN") == 1)) %>%
  ungroup() %>%
  arrange(desc(key_day), source)
```

## Summary tables 

```{r merge_report, echo=FALSE, message=FALSE, warning=FALSE}
cat("\n=== Inputs ===\n")
print(
  tibble::tibble(
    dataset = c("GBIF (clean)", "NBN (clean)"),
    rows    = c(nrow(gbif_clean), nrow(nbn_clean))
  )
)

cat("\n=== Merged totals ===\n")
print(
  tibble::tibble(
    stage = c("Merged (all)", "Merged (after strict dedup)"),
    rows  = c(nrow(occ_merged_all), nrow(occ_merged_dedup_strict))
  )
)

cat("\n=== Date precision by source ===\n")
occ_merged_all %>%
  count(source, date_precision, sort = TRUE) %>%
  print(n = Inf)

cat("\n=== Cross-source duplicate flags (how many records are involved) ===\n")
occ_merged_all %>%
  summarise(
    n_total = n(),
    n_cross_dup_day = sum(cross_dup_day, na.rm = TRUE),
    n_cross_dup_year_loose = sum(cross_dup_year_loose, na.rm = TRUE),
    prop_cross_dup_day = n_cross_dup_day / n_total,
    prop_cross_dup_year_loose = n_cross_dup_year_loose / n_total
  ) %>%
  print()

cat("\n=== What got auto-dropped under the strict rule ===\n")
print(tibble::tibble(
  prefer_source = prefer_source,
  dropped_rows  = nrow(dropped_strict),
  dropped_prop_of_merged = nrow(dropped_strict) / nrow(occ_merged_all),
  remaining_rows = nrow(occ_merged_dedup_strict)
))

if (nrow(dropped_strict) > 0) {
  cat("\nDropped rows by source/date precision:\n")
  dropped_strict %>% count(source, date_precision, sort = TRUE) %>% print(n = Inf)

  cat("\nDropped rows by licence:\n")
  dropped_strict %>% count(licence, sort = TRUE) %>% print(n = Inf)

  cat("\nTop years in dropped rows:\n")
  dropped_strict %>% count(year, sort = TRUE) %>% head(15) %>% print(n = Inf)
}

cat("\n=== Duplicate group structure (strict day keys) ===\n")
day_key_summary %>%
  filter(has_both_sources) %>%
  count(n_total, sort = TRUE) %>%
  print(n = Inf)

cat("\nAmbiguous cross-source groups kept for review: ",
    n_distinct(occ_crossdup_review$key_day), " keys (",
    nrow(occ_crossdup_review), " rows)\n", sep = "")
```

```{r merge_examples, echo=FALSE, message=FALSE, warning=FALSE}
cat("### Examples of ambiguous cross-source groups (kept)\n\n")

example_keys <- occ_crossdup_review %>%
  distinct(key_day) %>%
  head(3) %>%
  pull(key_day)

if (length(example_keys) == 0) {
  cat("No ambiguous groups in this run (all strict day matches were 1-to-1).\n")
} else {
  ex_tbl <- occ_crossdup_review %>%
    filter(key_day %in% example_keys) %>%
    select(key_day, source, record_id, occurrenceID, lon, lat, date_raw, date_precision, year, licence) %>%
    arrange(key_day, source)

  knitr::kable(ex_tbl)
}
```

## What we removed in this test run (and at what resolution)

This is a workflow test for a single species (*Vespula vulgaris*) to confirm we can pull, standardise, and merge occurrence data from **GBIF** and the **NBN Atlas** without obvious double-counting, while keeping the matching rules conservative and easy to explain.

### A) Source-specific screening (before any de-duplication)

For both sources we keep only records that meet two basic requirements:

- **Coordinates present:** `lon` and `lat` are not missing.
- **Acceptable licence:** GBIF licences are restricted to `allowed_licences` (CC0/CC-BY/CC-BY-NC) and NBN licences to `allowed_licences_nbn` (OGL/CC0/CC-BY/CC-BY-NC).

In this test run:

- GBIF (raw-with-coordinates, Europe scope) started at **62,421** records and all were within the accepted GBIF licence set in this run.
- NBN (screened UK-only pull) produced **11,682** records after coordinate + licence screening.

### B) Within-source de-duplication (exact matches only)

Before combining sources, we remove **exact repeats within each dataset**. This is intentionally light-touch: it does not try to collapse “near-duplicates”.

GBIF (`gbif_clean`):

- Step 1: remove duplicate `gbifID` (exact same GBIF record repeated).
  - **Resolution:** exact match on `gbifID`.
- Step 2: remove exact repeats of `(lon, lat, date)` after screening.
  - **Resolution:** exact equality on `lon`, exact equality on `lat`, and exact string equality on `date` (as stored in `gbif_clean$date`).

Net effect:

- GBIF: **62,421 → 56,323** (so **6,098** GBIF records were dropped as within-source duplicates).

NBN (`nbn_clean`):

- Step 1: remove duplicate `recordID`.
  - **Resolution:** exact match on `recordID`.
- Step 2: remove exact repeats of `(lon, lat, date)`.
  - **Resolution:** exact equality on `lon`, exact equality on `lat`, and exact equality on `date` (as stored in `nbn_clean$date`).

Net effect:

- NBN: **11,682 → 11,682** (no within-source duplicates removed in this run).

### C) Cross-source duplicate handling (GBIF vs NBN; approximate matching)

GBIF and NBN do not share a universal record identifier, so cross-source duplicate detection must be approximate. We construct matching keys in `occ_merged_all` and use them to *flag* possible duplicates; we only *drop* records in the simplest and safest case.

Strict “day-level” duplicate key (used for auto-drop when one-to-one):

- **Species:** exact match on `species` (here, always the test species).
- **Location:** coordinates rounded to **4 decimal places**:
  - `lon_key = round(lon, 4)`
  - `lat_key = round(lat, 4)`
  - (4 dp is ~10–11 m at UK latitudes; exact metres vary slightly with latitude.)
- **Date:** must have an unambiguous day:
  - `date_day_str = substr(date_raw, 1, 10)`
  - `date_day = ymd(date_day_str)` must succeed
  - date must **not** be a range (we treat `"YYYY-MM-DD/YYYY-MM-DD"` as a range and do not use it as a strict day key)

This builds `key_day` of the form:

- `species|<lon_4dp>|<lat_4dp>|<YYYY-MM-DD>`

Loose “year-level” key (used only for flagging; never auto-dropped):

- Same `species` and same rounded coordinates (4 dp), but match on `year` only:
  - `key_year = species|<lon_4dp>|<lat_4dp>|<YYYY>`

**What we automatically remove (conservative rule):**

- We only auto-drop records where the strict `key_day` produces a clean **one-to-one** match:
  - exactly **1 GBIF** record and exactly **1 NBN** record share the same `key_day`.
- In that case we keep the record from the preferred source (`prefer_source`, set to `"GBIF"` in this run) and drop the other record (the NBN one) to avoid double-counting.

**What we do not automatically remove:**

- Any “messy” clusters (e.g., 3+ records sharing the same `key_day`, or multiple GBIF and/or multiple NBN records sharing a key) are **not dropped**. They are retained and flagged for review in `occ_crossdup_review` because they could be:
  - genuine repeated observations (multiple observers, repeated sampling), or
  - artefacts of rounding being too coarse in dense datasets.

Net effect in this test run:

- Inputs: GBIF clean **56,323** (`gbif_clean`), NBN clean **11,682** (`nbn_clean`)
- Merged total (pre cross-source de-dup): **68,005** (`occ_merged_all`)
- Auto-dropped as strict one-to-one duplicates: **10,060** records (all were NBN day-level records)
  - these were removed because they matched GBIF on the strict `key_day` (same species, same rounded location to 4 dp, same parsed calendar day)
- Merged after strict de-dup: **57,945** (`occ_merged_dedup_strict`)

Practical interpretation for this species:

- A large fraction of NBN day-level records appear to be duplicates of GBIF under the conservative matching rule.
- We are deliberately conservative: we only delete where the duplication signal is strongest (one-to-one day matches), and we keep ambiguous clusters for review rather than risk deleting true observations.
