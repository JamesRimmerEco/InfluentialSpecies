---
title: "Influential species mapping - test full pull"
author: "James EV Rimmer"
date: "2026-01-15"
output:
  html_document:
    toc: true
    toc_depth: 3
---

### Introduction
This script is a full test of the start of a production workflow which builds on code tested in the "GBIF_NBN test" script.R" script. It pulls occurrence data for a single test species and applies basic screening so that tables of data extracted from **GBIF** and **NBN** can be later combined and ready to be imported to the Google Earth Engine in later steps. 

**Species test case:** *Vespula vulgaris* (common wasp)

**Goal (GBIF section):**
- demonstrate pulling data through the GBIF API (without an arbitrary record cap)
- apply basic screening: geography, coordinate presence, accepted licence set
- apply basic within-source de-duplication (lat/lon, ID)

**Accepted licences (GBIF records):**
- **CC0_1_0** – Public Domain Dedication
- **CC_BY_4_0** – Attribution 4.0
- **CC_BY_NC_4_0** – Attribution–NonCommercial 4.0

### Packages

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rgbif)
library(dplyr)
library(readr)
library(stringr)
library(lubridate)
```

### Settings

We set the species, geographic scope, and accepted licences for GBIF records.  
For GBIF downloads we  page through multiple API calls (batch pulls) so we can retrieve all records without an arbitrary record cap.

```{r}
species_name <- "Vespula vulgaris"      # scientific name
region_scope <- "EUROPE"               # GBIF continent filter
pause_s      <- 0.25                   # delay to reduce rate-limit risk

allowed_licences <- c("CC0_1_0", "CC_BY_4_0", "CC_BY_NC_4_0")  # adjust if needed

slug <- stringr::str_replace_all(tolower(species_name), "[^a-z0-9]+", "_") %>% 
  stringr::str_replace_all("^_+|_+$", "") # trim leading/trailing underscores
 # make a safe file-friendly species label (used in filenames)

# Output dirs (relative to scripts/)
gbif_out_dir <- file.path("..", "data", "raw", "gbif", "wasps")
ckpt_root    <- file.path("..", "data", "_checkpoints")  # shared checkpoint root (outside data/raw/)
gbif_ckpt_dir <- file.path(ckpt_root, "gbif")

# GBIF batch pull settings:
# - page_size controls the size of each API call (not a cap on total records)
# - max_records is a safety brake during development; set Inf for no cap
page_size   <- 1000
max_records <- Inf

```

## GBIF taxon resolution 
We first resolve the species name against the GBIF "backbone" so we can be sure we have a stable taxon identifier.

```{r}
# Purpose: resolve species name to a stable GBIF usageKey (avoid uncertain matching later)
bb <- name_backbone(name = species_name)

taxon_key <- bb$usageKey
message("GBIF match: ", bb$scientificName, " (usageKey=", taxon_key, ", matchType=", bb$matchType, ")")

# Display key info in the report
tibble(
  query = species_name,
  matched_name = bb$scientificName,
  usageKey = taxon_key,
  matchType = bb$matchType
)

```

## GBIF occurrence pull
We now pull all GBIF occurrence records for this taxon within the chosen geographic area.  
GBIF returns results in pages, so we iterate over pages until we have pulled the full set.

A small “record check” is included to confirm which licence strings are present in the raw pull before we filter.


```{r}
# Purpose: pull all GBIF occurrences for the species within the chosen scope (see settings above) 

# Checkpoint + retry settings (for long pulls)
dir.create(gbif_out_dir, recursive = TRUE, showWarnings = FALSE)
dir.create(gbif_ckpt_dir, recursive = TRUE, showWarnings = FALSE)

ckpt_file <- file.path(gbif_ckpt_dir, paste0("gbif_pull_checkpoint_", slug, ".rds"))
 # where we save progress for resuming

max_retries <- 5          # how many times to retry a failed page request before giving up
retry_base_wait_s <- 10   # base wait between retries; multiplies by attempt number (10s, 20s, 30s, ...)

start <- 0                     # record offset for paging through GBIF results (0 = start of result set)
all_pages <- list()            # container to store each page of results (combined at the end)
total_expected <- NA_integer_  # total number of matching records (read from GBIF meta on first successful call)

# If a checkpoint exists, resume instead of starting from scratch
if (file.exists(ckpt_file)) {
  ckpt <- readRDS(ckpt_file)            # load previously saved progress
  start <- ckpt$start                   # resume from the last recorded offset
  all_pages <- ckpt$all_pages           # keep pages already downloaded
  total_expected <- ckpt$total_expected # retain expected total for progress tracking

  message("Resuming GBIF pull from start = ", start,
          " (pages already stored: ", length(all_pages), ").")
}

repeat {
  Sys.sleep(pause_s) # small pause to avoid hammering the GBIF API

  # ---- Page request with retries (handles transient curl/network drops) ----
  res <- NULL
  for (attempt in seq_len(max_retries)) {
    res <- tryCatch(
      occ_search(
        taxonKey = taxon_key,        # stable GBIF identifier for the species
        continent = region_scope,    # restrict records to the chosen geographic scope
        hasCoordinate = TRUE,        # only return records with spatial coordinates (required for mapping + later dedup)
        limit = page_size,           # number of records per API call (page size; not a cap on total records)
        start = start                # offset for this page within the full result set
      ),
      error = function(e) e          # capture the error so we can retry instead of crashing immediately
    )

    if (!inherits(res, "error")) break  # success: exit retry loop and continue

    wait_s <- retry_base_wait_s * attempt # increasing backoff gives the API/network time to recover
    message("GBIF request failed at start = ", start,
            " (attempt ", attempt, "/", max_retries, "): ",
            conditionMessage(res),
            " | waiting ", wait_s, "s then retrying...")
    Sys.sleep(wait_s)
  }

  # If it still fails after retries, save progress and stop (so we can resume later)
  if (inherits(res, "error")) {
    saveRDS(list(start = start, all_pages = all_pages, total_expected = total_expected), ckpt_file)
    stop(res)
  }

  # Total number of records that match the query in GBIF (across all pages); captured once for progress tracking
  if (is.na(total_expected)) total_expected <- res$meta$count

  # Safety check: stop paging if the API returns an empty page (no records)
  if (length(res$data) == 0) break

  all_pages[[length(all_pages) + 1]] <- res$data  # append this page of records to our list of pages

  # Save progress after each successful page so we don't lose work if the session/connection dies
  saveRDS(list(start = start, all_pages = all_pages, total_expected = total_expected), ckpt_file)

  pulled_so_far <- start + nrow(res$data) # number pulled so far = start offset + rows returned in this page
  message("Pulled ", pulled_so_far, " / ", total_expected, " rows...")

  # Stop when we have everything (or hit the development safety brake)
  if (pulled_so_far >= total_expected) break
  if (pulled_so_far >= max_records) {
    message("Reached max_records safety limit (", max_records, ").")
    break
  }

  start <- start + nrow(res$data) # advance by what we actually received (avoids skipping if a page is short)
}

gbif_raw <- bind_rows(all_pages) # combine all pages into one data frame for downstream cleaning/screening

message("GBIF expected rows (query): ", total_expected)
message("GBIF rows pulled (paged):  ", nrow(gbif_raw))

```

## Record check (raw pull)

Before any screening, we check:
- the licence strings present in the raw pull (as GBIF supplies them)
- a quick glimpse of key fields, just to confirm the structure is as expected

```{r}
# Inspect what we pulled before filtering (including licence strings)

# Licence strings as supplied by GBIF (typically URLs)
gbif_raw %>%
  count(license, sort = TRUE) %>%
  mutate(prop = n / sum(n)) %>%
  print(n = Inf)

# Quick structure check on key fields
gbif_raw %>%
  select(gbifID, occurrenceID, decimalLongitude, decimalLatitude, eventDate, year, countryCode, license) %>%
  head(10)

```

## Basic screening + essential fields

GBIF supplies licence information as URLs. We modify these URLs to short recognisable codes and then filter to an accepted set (this might not filter in practice if all licenses are acceptable).   
We also keep only the essential fields we’ll need later (including for cross-source duplicate checks and GEE import).

```{r}
# Purpose: standardise essential fields and apply basic screens (coords + accepted licence set)

# Licence normaliser (GBIF uses URLs with 'licenses' in the path)
lic_normalise <- function(x) {
  dplyr::case_when(
    stringr::str_detect(x, "publicdomain/zero/1.0") ~ "CC0_1_0",
    stringr::str_detect(x, "licenses/by/4.0") ~ "CC_BY_4_0",
    stringr::str_detect(x, "licenses/by-nc/4.0") ~ "CC_BY_NC_4_0",
    TRUE ~ NA_character_
  )
}

gbif_clean <- gbif_raw %>%
  transmute(
    source = "GBIF",
    species = species_name,
    gbifID = gbifID,
    occurrenceID = occurrenceID,
    lon = decimalLongitude,
    lat = decimalLatitude,
    date = eventDate,
    year = year,
    country = countryCode,
    licence_raw = license,
    licence = lic_normalise(license)
  ) %>%
  filter(!is.na(lon), !is.na(lat)) %>%
  filter(!is.na(licence) & licence %in% allowed_licences)

message("GBIF rows (raw -> screened): ", nrow(gbif_raw), " -> ", nrow(gbif_clean))

# Quick summaries
gbif_clean %>% count(licence, sort = TRUE)
gbif_clean %>% count(country, sort = TRUE) %>% head(10)


```

## De-duplication

We remove obvious within-source duplicates:
1) duplicate `gbifID` (the same GBIF record repeated)
2) identical `(lon, lat, date)` combinations (a light redundancy screen)

This helps keep the table tidy before we later combine GBIF with NBN and screen cross-source duplicates.

```{r}
# Purpose: light within-source de-duplication before later cross-source merging

n_before <- nrow(gbif_clean)

gbif_clean <- gbif_clean %>%
  distinct(gbifID, .keep_all = TRUE) %>%
  distinct(lon, lat, date, .keep_all = TRUE)

message("GBIF rows (screened -> de-dup): ", n_before, " -> ", nrow(gbif_clean))

```
## Save output

We save the screened, de-duplicated GBIF table so we can reuse it without re-pulling from the API.

```{r}
# Purpose: save clean GBIF table for reuse
gbif_outfile <- file.path(gbif_out_dir, paste0("gbif_", slug, "_clean.csv"))

write_csv(gbif_clean, gbif_outfile)
gbif_outfile

```


## GBIF summary

At this point we have:
- resolved *Vespula vulgaris* to a stable GBIF identifier
- pulled all GBIF occurrence records in the European scope
- checked raw licence strings and record structure before screening
- standardised essential fields, filtered to accepted licences and records with coordinates
- applied light de-duplication by coordinates and ID
- optionally saved a clean CSV for reuse

# ============================================================
# NBN (UK) occurrence pull (galah / UK atlas)

```{r}
library(galah)

# NBN access: use the email associated with your atlas account
nbn_email <- "jamesrimmer92@mail.com"

# Accepted licences (NBN tends to return short codes like CC-BY, CC0, etc.)
allowed_licences_nbn <- c("OGL", "CC0", "CC-BY", "CC-BY-NC")

# Output dirs (relative to scripts/)
nbn_out_dir  <- file.path("..", "data", "raw", "nbn", "wasps")
nbn_ckpt_dir <- file.path(ckpt_root, "nbn")

# Configure galah to use the UK atlas + provide a download reason
galah_config(atlas = "United Kingdom", email = nbn_email, verbose = FALSE)
galah_config(download_reason_id = 17) # 17 = research/academic purposes (as per your earlier working setup)
```

## NBN taxon check

We first confirm that the species name resolves cleanly in the UK atlas.

```{r}
nbn_taxa <- search_taxa(species_name)

message("NBN taxon search (top hit):")
nbn_taxa %>%
  dplyr::select(scientific_name, taxon_concept_id, rank) %>%
  head(1)
```

## NBN occurrence pull

We now pull all occurrences for the species from the UK atlas.  
(We keep the field selection minimal and consistent with GBIF so later merging is straightforward.)

```{r}
dir.create(nbn_out_dir,  recursive = TRUE, showWarnings = FALSE)
dir.create(nbn_ckpt_dir, recursive = TRUE, showWarnings = FALSE)

nbn_outfile <- file.path(nbn_out_dir, paste0("nbn_", slug, "_clean.csv"))

# Checkpoint file for NBN raw pull (so we can resume without re-downloading)
nbn_ckpt_file <- file.path(nbn_ckpt_dir, paste0("nbn_pull_checkpoint_", slug, ".rds"))

max_retries <- 5         # how many times to retry a failed request before giving up
retry_base_wait_s <- 10  # base wait between retries; multiplies by attempt number (10s, 20s, 30s, ...)

# If we've already saved a clean file, reuse it (avoids re-pulling from the API)
if (file.exists(nbn_outfile)) {
  message("Found existing NBN clean file, reading: ", nbn_outfile)
  nbn_clean <- readr::read_csv(nbn_outfile, show_col_types = FALSE)

} else {

  # If a checkpoint exists, reuse the raw pull from disk (avoid re-downloading)
  if (file.exists(nbn_ckpt_file)) {
    message("Found NBN checkpoint, loading: ", nbn_ckpt_file)
    nbn_raw <- readRDS(nbn_ckpt_file)

  } else {

    # Pull raw occurrences (with simple retries for transient network/API issues)
    nbn_raw <- NULL
    for (attempt in seq_len(max_retries)) {

      nbn_raw <- tryCatch(
        galah_call() |>
          galah_identify(species_name) |>
          atlas_occurrences(
            select = galah_select(
              recordID,
              scientificName,
              eventDate,
              year,
              decimalLatitude,
              decimalLongitude,
              license  # request field as 'license' (returned column name is usually `dcterms:license`)
            )
          ),
        error = function(e) e
      )

      if (!inherits(nbn_raw, "error")) break  # success

      wait_s <- retry_base_wait_s * attempt
      message("NBN request failed (attempt ", attempt, "/", max_retries, "): ",
              conditionMessage(nbn_raw),
              " | waiting ", wait_s, "s then retrying...")
      Sys.sleep(wait_s)
    }

    # If it still fails after retries, stop
    if (inherits(nbn_raw, "error")) stop(nbn_raw)

    # Save raw pull to checkpoint so we don't need to re-download next time
    saveRDS(nbn_raw, nbn_ckpt_file)
    message("Saved NBN checkpoint: ", nbn_ckpt_file)
  }

  message("NBN raw rows: ", nrow(nbn_raw))
}

```

## Record check + cleaning (NBN)

```{r}
# Only run if we have raw NBN data (i.e. we didn't already load a clean CSV)
if (exists("nbn_raw") && !exists("nbn_clean")) {

  lic_col <- if ("dcterms:license" %in% names(nbn_raw)) "dcterms:license" else "license"

  # Record check: licence strings as returned by NBN Atlas
  message("NBN licence breakdown (RAW pull):")
  nbn_raw %>%
    dplyr::count(.data[[lic_col]], sort = TRUE) %>%
    dplyr::mutate(prop = n / sum(n)) %>%
    print(n = Inf)

  # Normalise NBN licence strings to short codes
  lic_normalise_nbn <- function(x) {
    x_l <- stringr::str_to_lower(x)

    dplyr::case_when(
      is.na(x) ~ NA_character_,
      x %in% c("OGL", "CC0", "CC-BY", "CC-BY-NC") ~ x,
      stringr::str_detect(x_l, "open government licence|\\bogl\\b") ~ "OGL",
      stringr::str_detect(x_l, "\\bcc0\\b|publicdomain/zero/1.0") ~ "CC0",
      stringr::str_detect(x_l, "cc[- ]?by[- ]?nc|licenses/by-nc/4.0") ~ "CC-BY-NC",
      stringr::str_detect(x_l, "cc[- ]?by\\b|licenses/by/4.0") ~ "CC-BY",
      TRUE ~ NA_character_
    )
  }

  # Create nbn_clean in the same column style as gbif_clean
  nbn_clean <- nbn_raw %>%
    dplyr::transmute(
      source = "NBN",
      species = species_name,
      recordID = recordID,
      lon = decimalLongitude,
      lat = decimalLatitude,
      date = eventDate,
      year = year,
      licence_raw = .data[[lic_col]],
      licence = lic_normalise_nbn(.data[[lic_col]])
    ) %>%
    dplyr::filter(!is.na(lon), !is.na(lat)) %>%
    dplyr::filter(!is.na(licence) & licence %in% allowed_licences_nbn)
  
  message("NBN rows (raw -> screened): ", nrow(nbn_raw), " -> ", nrow(nbn_clean))
}
```

## De-duplication (NBN)

As with GBIF, we apply de-duplication to remove identical reports by ID or by co-ordinate and date.

```{r}
if (exists("nbn_clean")) {
  n_before <- nrow(nbn_clean)

  nbn_clean <- nbn_clean %>%
    dplyr::distinct(recordID, .keep_all = TRUE) %>%      # remove exact duplicates by record ID
    dplyr::distinct(lon, lat, date, .keep_all = TRUE)    # remove identical lon/lat/date duplicates

  message("NBN rows (screened -> de-dup): ", n_before, " -> ", nrow(nbn_clean))
}
```

## Save output (NBN)

Save a clean NBN CSV alongside the GBIF outputs.

```{r nbn_save_output}
if (exists("nbn_clean")) {
  write_csv(nbn_clean, nbn_outfile)
  nbn_outfile
}
```

## NBN summary

At this point we have:
- resolved *Vespula vulgaris* in the UK atlas
- pulled NBN occurrence records
- checked raw licence strings before screening
- standardised essential fields, filtered to accepted licences and usable coordinates
- applied light de-duplication by coordinates + date, and ID
- saved a clean CSV for reuse

# ============================================================
## Merge GBIF + NBN and check cross-source duplicates

At this point we have two screened and within-source de-duplicated tables:

- `gbif_clean` (GBIF, Europe-wide)
- `nbn_clean` (NBN Atlas, UK-only)

The next step is to combine these into a single table and check for records that may be present in both sources. There is no shared universal record ID between GBIF and NBN, so the safest cross-source duplicate check has to be approximate.

### Checking for duplicates 

**What we use as a matching key**
- **Species name** (here, one test species)
- **Location** (lon/lat, rounded to a fixed number of decimal places)
- **Time**
  - *Strict:* same **day** (only when we can safely extract a day from the date string)
  - *Loose:* same **year** (used only as a flag, not an automatic de-duplicate)

**Why it’s approximate**
- Coordinates can be rounded differently between sources, or have small jitter.
- GBIF dates are not always a clean single day (they can be timestamps, ranges, or missing).
- Two different observers could genuinely record the same species at the same location on the same day. If we collapse those, we might be losing true separate records.

**Safety rule**
- We only remove cross-source duplicates automatically when the match is a simple one-to-one pair (exactly one GBIF record and one NBN record for the same strict day-level key).
- Anything messier than that is kept, but clearly flagged and printed so I can sanity check it. 

```{r}
# Purpose: stop early if the expected cleaned tables are not present
if (!exists("gbif_clean")) stop("gbif_clean not found - run the GBIF section first.")
if (!exists("nbn_clean")) stop("nbn_clean not found - run the NBN section first.")
```

## Standardise both tables into a common shape

We standardise column names so GBIF and NBN can be bound row-wise cleanly, while still keeping traceability back to each source.

```{r}
# Purpose: standardise GBIF and NBN into the same column set for merging

gbif_merge <- gbif_clean %>%
  transmute(
    source = source,                      # "GBIF"
    species = species,
    record_id = as.character(gbifID),     # source-specific unique ID (GBIF ID)
    occurrenceID = as.character(occurrenceID),
    lon = lon,
    lat = lat,
    date_raw = as.character(date),        # keep raw date string exactly as supplied
    year = as.integer(year),
    country = as.character(country),
    licence_raw = as.character(licence_raw),
    licence = as.character(licence)
  )

nbn_merge <- nbn_clean %>%
  transmute(
    source = source,                      # "NBN"
    species = species,
    record_id = as.character(recordID),   # source-specific unique ID (NBN recordID)
    occurrenceID = NA_character_,         # not available for NBN in our pull (could change this)
    lon = lon,
    lat = lat,
    date_raw = as.character(date),        # NBN dates are usually already day-level
    year = as.integer(year),
    country = "GB",                       # NBN Atlas is UK-only; set a consistent country label for merging
    licence_raw = as.character(licence_raw),
    licence = as.character(licence)
  )

# Combine into one table (still contains duplicates, if any)
occ_all <- bind_rows(gbif_merge, nbn_merge)

message("Rows before merge: GBIF=", nrow(gbif_merge), " | NBN=", nrow(nbn_merge))
message("Rows after merge (no cross-source de-duplicating yet): ", nrow(occ_all))
```

## Derive date precision and create duplicate-check keys

GBIF `eventDate` is not always a single day, so we only treat a date as “day-level” if it clearly starts with `YYYY-MM-DD` and is not a range (e.g. contains `/`).

We also round coordinates to a fixed number of decimal places so small differences don’t prevent matching. This is a judgement call: tighter rounding = fewer false matches but more missed duplicates; looser rounding = more matches but greater risk of collapsing genuinely distinct points.

```{r}
# Purpose: create strict (day-level) and loose (year-level) matching keys

coord_round_dp <- 4  # 4 dp ~ ~10 m; increase to be stricter, decrease to be more tolerant

occ_all <- occ_all %>%
  mutate(
    # Round lon/lat and format as fixed-width strings for stable key construction
    lon_key = if_else(is.na(lon), NA_character_,
                      sprintf(paste0("%.", coord_round_dp, "f"), round(lon, coord_round_dp))),
    lat_key = if_else(is.na(lat), NA_character_,
                      sprintf(paste0("%.", coord_round_dp, "f"), round(lat, coord_round_dp))),

    # Identify obvious date ranges (GBIF sometimes returns intervals like "2015-01-01/2015-12-31")
    date_is_range = !is.na(date_raw) & str_detect(date_raw, "/"),

    # Extract a day string only when it looks like an ISO day and isn't a range
    date_day_str = if_else(
      !is.na(date_raw) & !date_is_range & str_detect(date_raw, "^\\d{4}-\\d{2}-\\d{2}"),
      substr(date_raw, 1, 10),
      NA_character_
    ),

    # Parse to Date (quiet=TRUE avoids warnings for odd strings; returns NA if it can't parse)
    date_day = lubridate::ymd(date_day_str, quiet = TRUE),

    # Track what precision we actually have available
    date_precision = case_when(
      !is.na(date_day) ~ "day",
      !is.na(year) ~ "year",
      TRUE ~ "unknown"
    ),

    # Strict key: same species + rounded coords + same day (only when day is available)
    key_day = if_else(
      date_precision == "day",
      paste(species, lon_key, lat_key, as.character(date_day), sep = "|"),
      NA_character_
    ),

    # Loose key: same species + rounded coords + same year (used for flagging only)
    key_year = if_else(
      !is.na(year),
      paste(species, lon_key, lat_key, as.character(year), sep = "|"),
      NA_character_
    )
  )

# Quick check: what date precision do we actually have by source?
occ_all %>%
  count(source, date_precision, sort = TRUE) %>%
  print(n = Inf)
```

## Cross-source duplicate check (strict day-level)

This identifies candidate duplicates where both sources have at least one record with the same strict day-level key.

These are the strongest candidates for “same observation listed twice”, but they are not guaranteed duplicates.

```{r}
# Purpose: find strict cross-source duplicate groups (same day + rounded coords)

dup_day_groups <- occ_all %>%
  filter(!is.na(key_day)) %>%                 # strict matching only possible when a day-level date exists
  group_by(key_day) %>%
  summarise(
    n_total = n(),
    n_gbif = sum(source == "GBIF"),
    n_nbn  = sum(source == "NBN"),
    .groups = "drop"
  ) %>%
  filter(n_gbif > 0 & n_nbn > 0) %>%          # must appear in both sources
  arrange(desc(n_total))

message("Strict cross-source duplicate groups (day-level): ", nrow(dup_day_groups))
message("Strict cross-source duplicate records involved (day-level): ", sum(dup_day_groups$n_total))

# Distribution of group sizes (helpful to spot messy cases where rounding may be too loose)
dup_day_groups %>%
  count(n_total, sort = TRUE) %>%
  print(n = Inf)

# Show a small sample of candidate duplicates for sanity checking
if (nrow(dup_day_groups) > 0) {
  example_keys <- head(dup_day_groups$key_day, 3)

  occ_all %>%
    filter(key_day %in% example_keys) %>%
    arrange(key_day, source) %>%
    select(source, record_id, occurrenceID, lon, lat, date_raw, date_day, year, country, licence) %>%
    print(n = Inf)
}
```

## Cross-source duplicate check (loose year-level only)

This flags cases where both sources have records in the same rounded location and year, but at least one record lacks a usable day-level date. These are possible duplicates, but are much less certain, so we do not remove them automatically.

```{r}
# Purpose: flag loose cross-source duplicates (same year + rounded coords), used for review only

dup_year_loose_groups <- occ_all %>%
  filter(!is.na(key_year)) %>%
  group_by(key_year) %>%
  summarise(
    n_total = n(),
    n_gbif = sum(source == "GBIF"),
    n_nbn  = sum(source == "NBN"),
    any_not_day = any(date_precision != "day"),  # at least one record only has year/unknown precision
    .groups = "drop"
  ) %>%
  filter(n_gbif > 0 & n_nbn > 0 & any_not_day) %>%
  arrange(desc(n_total))

message("Loose cross-source duplicate groups (year-level, at least one record not day-precise): ", nrow(dup_year_loose_groups))

# Show a small sample for inspection (these are the ones most likely to be ambiguous)
if (nrow(dup_year_loose_groups) > 0) {
  example_keys <- head(dup_year_loose_groups$key_year, 3)

  occ_all %>%
    filter(key_year %in% example_keys) %>%
    arrange(key_year, source) %>%
    select(source, record_id, occurrenceID, lon, lat, date_raw, date_precision, year, country, licence) %>%
    print(n = Inf)
}
```

## Build merged outputs and apply conservative cross-source de-duplication

Here we produce two outputs:

1) `occ_merged_all`  
   Everything combined, with flags showing which records are part of strict/loose cross-source duplicate groups.

2) `occ_merged_dedup_strict`  
   A conservative de-duplicated version where we remove only clear one-to-one strict duplicates.

**Important:** We only drop records for strict groups that are exactly one GBIF + one NBN record. If a group is larger than that, it stays in the data and is flagged for review (because rounding could be collapsing nearby-but-distinct points, or there could be multiple genuine observations).

```{r}
# Purpose: flag duplicates and create a conservative de-duplicated table

# Mark strict day-level duplicate membership
strict_keys <- dup_day_groups$key_day

# Mark loose year-level duplicate membership
loose_year_keys <- dup_year_loose_groups$key_year

occ_merged_all <- occ_all %>%
  mutate(
    cross_dup_day = !is.na(key_day) & key_day %in% strict_keys,
    cross_dup_year_loose = !is.na(key_year) & key_year %in% loose_year_keys
  )

# Identify strictly one-to-one groups only (the safest automatic de-duplicate case)
one_to_one_keys <- dup_day_groups %>%
  filter(n_total == 2 & n_gbif == 1 & n_nbn == 1) %>%
  pull(key_day)

message("Strict one-to-one duplicate groups (safe auto de-duplicate): ", length(one_to_one_keys))

# Conservative de-duplicate rule:
# - For one-to-one strict duplicates, keep the GBIF record and drop the NBN record
#   (this is arbitrary; we could flip the condition below)
occ_merged_dedup_strict <- occ_merged_all %>%
  mutate(
    drop = !is.na(key_day) & key_day %in% one_to_one_keys & source == "NBN"
  ) %>%
  filter(!drop) %>%
  select(-drop)

message("Rows in merged (all):          ", nrow(occ_merged_all))
message("Rows in merged (dedup strict): ", nrow(occ_merged_dedup_strict))
message("Rows dropped by strict de-dup: ", nrow(occ_merged_all) - nrow(occ_merged_dedup_strict))

# Safety check: show any strict duplicate groups that are NOT one-to-one (these are left in place intentionally)
dup_day_risky <- dup_day_groups %>%
  filter(!(n_total == 2 & n_gbif == 1 & n_nbn == 1))

message("Strict cross-source groups retained for review (not one-to-one): ", nrow(dup_day_risky))

if (nrow(dup_day_risky) > 0) {
  print(head(dup_day_risky, 10), n = Inf)
}
```

## Save merged outputs

We save both versions so we can decide later whether to use the conservative de-duplicated table, or work from the “all records” version with duplicate flags.

```{r}
# Purpose: save merged tables (all + conservative strict de-dup)

merge_out_dir <- file.path("..", "data", "processed", "merged", "wasps")
dir.create(merge_out_dir, recursive = TRUE, showWarnings = FALSE)

merged_all_outfile <- file.path(merge_out_dir, paste0("merged_gbif_nbn_", slug, "_all.csv"))
merged_dedup_outfile <- file.path(merge_out_dir, paste0("merged_gbif_nbn_", slug, "_dedup_strict.csv"))

write_csv(occ_merged_all, merged_all_outfile)
write_csv(occ_merged_dedup_strict, merged_dedup_outfile)

merged_all_outfile
merged_dedup_outfile
```

## Merge summary

At this point we have:
- a combined GBIF+NBN table (`occ_merged_all`) with flags for potential cross-source duplicates
- a conservative de-duplicated version (`occ_merged_dedup_strict`) where only one-to-one strict duplicates are removed
- printed summaries showing:
  - how many strict day-level duplicate groups exist
  - whether any groups look “messy” (which can indicate coordinate rounding is too loose, or dense repeated records)

The loose year-level duplicate groups are retained and only flagged, because they are much more likely to be ambiguous.



