---
title: "Influential species mapping - test full pull"
author: "James EV Rimmer"
date: "2026-01-15"
output:
  html_document:
    toc: true
    toc_depth: 3
---

### Introduction
This script is a full test of the start of a production workflow which builds on code tested in the "GBIF_NBN test" script.R" script. It pulls occurrence data for a single test species and applies basic screening so that tables of data extracted from **GBIF** and **NBN** can be later combined and ready to be imported to the Google Earth Engine in later steps. 

**Species test case:** *Vespula vulgaris* (common wasp)

**Goal (GBIF section):**
- demonstrate pulling data through the GBIF API (without an arbitrary record cap)
- apply basic screening: geography, coordinate presence, accepted licence set
- apply basic within-source de-duplication (lat/lon, ID)

**Accepted licences (GBIF records):**
- **CC0_1_0** – Public Domain Dedication
- **CC_BY_4_0** – Attribution 4.0
- **CC_BY_NC_4_0** – Attribution–NonCommercial 4.0

### Packages

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rgbif)
library(dplyr)
library(readr)
library(stringr)
library(lubridate)
```

### Settings

We set the species, geographic scope, and accepted licences for GBIF records.  
For GBIF downloads we use paging (multiple API calls) so we can retrieve all records without a cap.

```{r}
species_name <- "Vespula vulgaris"      # scientific name
region_scope <- "EUROPE"               # GBIF continent filter
pause_s      <- 0.25                   # delay to reduce rate-limit risk

allowed_licences <- c("CC0_1_0", "CC_BY_4_0", "CC_BY_NC_4_0")  # adjust if needed

# Output dirs (relative to scripts/)
gbif_out_dir <- file.path("..", "data", "raw", "gbif", "wasps")
gbif_ckpt_dir <- file.path("..", "data", "raw", "_checkpoints", "gbif")


# GBIF paging settings:
# - page_size controls the size of each API call (not a cap on total records)
# - max_records is a safety brake during development; set Inf for no cap
page_size   <- 1000
max_records <- Inf

```

## BIF taxon resolution 
We first resolve the species name against the GBIF "backbone" so we can be sure we have a stable taxon identifier.

```{r}
# Purpose: resolve species name to a stable GBIF usageKey (avoid uncertain matching later)
bb <- name_backbone(name = species_name)

taxon_key <- bb$usageKey
message("GBIF match: ", bb$scientificName, " (usageKey=", taxon_key, ", matchType=", bb$matchType, ")")

# Display key info in the report
tibble(
  query = species_name,
  matched_name = bb$scientificName,
  usageKey = taxon_key,
  matchType = bb$matchType
)

```

## GBIF occurence pull
We now pull all GBIF occurrence records for this taxon within the chosen geographic area.  
GBIF returns results in pages, so we iterate over pages until we have pulled the full set.

A small “record check” is included to confirm which licence strings are present in the raw pull before we filter.


```{r}
# Purpose: pull all GBIF occurrences for the species within the chosen scope (see settings above) 

# Checkpoint + retry settings (for long pulls)
dir.create(gbif_out_dir, recursive = TRUE, showWarnings = FALSE)
dir.create(gbif_ckpt_dir, recursive = TRUE, showWarnings = FALSE)


slug <- stringr::str_replace_all(tolower(species_name), "[^a-z0-9]+", "_") %>% 
  stringr::str_replace_all("^_+|_+$", "")  # trim leading/trailing underscores
 # make a safe file-friendly species label (used in filenames)

ckpt_file <- file.path(gbif_ckpt_dir, paste0("gbif_pull_checkpoint_", slug, ".rds"))
 # where we save progress for resuming

max_retries <- 5          # how many times to retry a failed page request before giving up
retry_base_wait_s <- 10   # base wait between retries; multiplies by attempt number (10s, 20s, 30s, ...)

start <- 0                     # record offset for paging through GBIF results (0 = start of result set)
all_pages <- list()            # container to store each page of results (combined at the end)
total_expected <- NA_integer_  # total number of matching records (read from GBIF meta on first successful call)

# If a checkpoint exists, resume instead of starting from scratch
if (file.exists(ckpt_file)) {
  ckpt <- readRDS(ckpt_file)            # load previously saved progress
  start <- ckpt$start                   # resume from the last recorded offset
  all_pages <- ckpt$all_pages           # keep pages already downloaded
  total_expected <- ckpt$total_expected # retain expected total for progress tracking

  message("Resuming GBIF pull from start = ", start,
          " (pages already stored: ", length(all_pages), ").")
}

repeat {
  Sys.sleep(pause_s) # small pause to avoid hammering the GBIF API

  # ---- Page request with retries (handles transient curl/network drops) ----
  res <- NULL
  for (attempt in seq_len(max_retries)) {
    res <- tryCatch(
      occ_search(
        taxonKey = taxon_key,        # stable GBIF identifier for the species
        continent = region_scope,    # restrict records to the chosen geographic scope
        hasCoordinate = TRUE,        # only return records with spatial coordinates (required for mapping + later dedup)
        limit = page_size,           # number of records per API call (page size; not a cap on total records)
        start = start                # offset for this page within the full result set
      ),
      error = function(e) e          # capture the error so we can retry instead of crashing immediately
    )

    if (!inherits(res, "error")) break  # success: exit retry loop and continue

    wait_s <- retry_base_wait_s * attempt # increasing backoff gives the API/network time to recover
    message("GBIF request failed at start = ", start,
            " (attempt ", attempt, "/", max_retries, "): ",
            conditionMessage(res),
            " | waiting ", wait_s, "s then retrying...")
    Sys.sleep(wait_s)
  }

  # If it still fails after retries, save progress and stop (so we can resume later)
  if (inherits(res, "error")) {
    saveRDS(list(start = start, all_pages = all_pages, total_expected = total_expected), ckpt_file)
    stop(res)
  }

  # Total number of records that match the query in GBIF (across all pages); captured once for progress tracking
  if (is.na(total_expected)) total_expected <- res$meta$count

  # Safety check: stop paging if the API returns an empty page (no records)
  if (length(res$data) == 0) break

  all_pages[[length(all_pages) + 1]] <- res$data  # append this page of records to our list of pages

  # Save progress after each successful page so we don't lose work if the session/connection dies
  saveRDS(list(start = start, all_pages = all_pages, total_expected = total_expected), ckpt_file)

  pulled_so_far <- start + nrow(res$data) # number pulled so far = start offset + rows returned in this page
  message("Pulled ", pulled_so_far, " / ", total_expected, " rows...")

  # Stop when we have everything (or hit the development safety brake)
  if (pulled_so_far >= total_expected) break
  if (pulled_so_far >= max_records) {
    message("Reached max_records safety limit (", max_records, ").")
    break
  }

  start <- start + nrow(res$data) # advance by what we actually received (avoids skipping if a page is short)
}

gbif_raw <- bind_rows(all_pages) # combine all pages into one data frame for downstream cleaning/screening

message("GBIF expected rows (query): ", total_expected)
message("GBIF rows pulled (paged):  ", nrow(gbif_raw))

```

## Record check (raw pull)

Before any screening, we check:
- the licence strings present in the raw pull (as GBIF supplies them)
- a quick glimpse of key fields, just to confirm the structure is as expected

```{r}
# Inspect what we pulled before filtering (including licence strings)

# Licence strings as supplied by GBIF (typically URLs)
gbif_raw %>%
  count(license, sort = TRUE) %>%
  mutate(prop = n / sum(n)) %>%
  print(n = Inf)

# Quick structure check on key fields
gbif_raw %>%
  select(gbifID, occurrenceID, decimalLongitude, decimalLatitude, eventDate, year, countryCode, license) %>%
  head(10)

```

## Basic screening + essential fields

GBIF supplies licence information as URLs. We modify these URLs to short recognisable codes and then filter to an accepted set (this might not filter in practice if all licenses are acceptable).   
We also keep only the essential fields we’ll need later (including for cross-source duplicate checks and GEE import).

```{r}
# Purpose: standardise essential fields and apply basic screens (coords + accepted licence set)

# Licence normaliser (GBIF uses URLs with 'licenses' in the path)
lic_normalise <- function(x) {
  dplyr::case_when(
    stringr::str_detect(x, "publicdomain/zero/1.0") ~ "CC0_1_0",
    stringr::str_detect(x, "licenses/by/4.0") ~ "CC_BY_4_0",
    stringr::str_detect(x, "licenses/by-nc/4.0") ~ "CC_BY_NC_4_0",
    TRUE ~ NA_character_
  )
}

gbif_clean <- gbif_raw %>%
  transmute(
    source = "GBIF",
    species = species_name,
    gbifID = gbifID,
    occurrenceID = occurrenceID,
    lon = decimalLongitude,
    lat = decimalLatitude,
    date = eventDate,
    year = year,
    country = countryCode,
    licence_raw = license,
    licence = lic_normalise(license)
  ) %>%
  filter(!is.na(lon), !is.na(lat)) %>%
  filter(!is.na(licence) & licence %in% allowed_licences)

message("GBIF rows (raw -> screened): ", nrow(gbif_raw), " -> ", nrow(gbif_clean))

# Quick summaries
gbif_clean %>% count(licence, sort = TRUE)
gbif_clean %>% count(country, sort = TRUE) %>% head(10)


```

## De-duplication

We remove obvious within-source duplicates:
1) duplicate `gbifID` (the same GBIF record repeated)
2) identical `(lon, lat, date)` combinations (a light redundancy screen)

This helps keep the table tidy before we later combine GBIF with NBN and screen cross-source duplicates.

```{r}
# Purpose: light within-source de-duplication before later cross-source merging

n_before <- nrow(gbif_clean)

gbif_clean <- gbif_clean %>%
  distinct(gbifID, .keep_all = TRUE) %>%
  distinct(lon, lat, date, .keep_all = TRUE)

message("GBIF rows (screened -> de-dup): ", n_before, " -> ", nrow(gbif_clean))

```
## Save output

We save the screened, de-duplicated GBIF table so we can reuse it without re-pulling from the API.

```{r}
# Purpose: save clean GBIF table for reuse
slug <- str_replace_all(tolower(species_name), "[^a-z0-9]+", "_") %>%
  str_replace_all("^_+|_+$", "")

gbif_outfile <- file.path(gbif_out_dir, paste0("gbif_", slug, "_clean.csv"))

write_csv(gbif_clean, gbif_outfile)
gbif_outfile

```


## GBIF summary

At this point we have:
- resolved *Vespula vulgaris* to a stable GBIF `usageKey`
- pulled all GBIF occurrence records in the European scope
- checked raw licence strings and record structure before screening
- standardised essential fields, filtered to accepted licences and usable coordinates
- applied light de-duplication by coordinates and ID
- optionally saved a clean CSV for reuse

